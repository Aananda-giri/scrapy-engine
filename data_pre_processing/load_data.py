# -*- coding: utf-8 -*-
"""load_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xBpQu5kaF8DqE_-It5hdjH5QcE-Ldo7q
"""

# Mount google drive
import os
from google.colab import drive
if not os.path.exists('/content/drive'):
    # Mount if not already mounted
    drive.mount('/content/drive')
else:
    print('already mounted')

# Get chunks of data 10000 paragraphs at a time

import csv
import json
import sys

# Increase the CSV field size limit
csv.field_size_limit(sys.maxsize) # otherwise it gives "Error: field larger than field limit (131072)"

def load_data(file_path, chunk_size=10000):
  """Reads a CSV file in chunks of specified size.

  Args:
    file_path: Path to the CSV file.
    chunk_size: Number of rows to read at a time.

  Yields:
    A list of rows for each chunk.
  """

  with open(file_path, 'r') as csvfile:
    reader = csv.reader(csvfile)

    # # Skip the header
    # next(reader)

    chunk = []
    first_row=True
    for row in reader:
      # paragraphs: convert back to list
      if not first_row:
          row[2] = json.loads(row[2])
      else:
          first_row=False
      chunk.append(row)
      if len(chunk) >= chunk_size:
        yield chunk
        chunk = []

    if chunk:  # Handle the last chunk if not empty
      yield chunk


if __name__ == '__main__':
    file_path = '/content/drive/MyDrive/Research/datasets/crawled_data/crawled_data.csv'
    chunk_size = 100  # 10000

    # Example usage:
    for chunk in load_data(file_path, chunk_size):
        # .............................................
        '''
        * code to Process each chunk of data here
        * each chunk is list of list.
        * format of inner list of chunk is is: ['parent_url', 'page_title', 'paragraph']

        e.g.
        chunk = [
          ['https://www.bbc.com/nepali','मुख पृष्ठ - BBC News नेपाली', 'सुर्खेत र जुम्लामा बाहेक कर्णालीका अरू जिल्लामा शिशुका लागि आवश्यक एनआईसीयू सेवा नै उपलब्ध छैन।'],
          ['https://www.bbc.com/nepali', 'मुख पृष्ठ - BBC News नेपाली', 'नेपालले करिब एक महिना अघि नै औपचारिक पत्र पठाएर जीबी राईलाई स्वदेश फर्काइदिन गरेको आग्रहबारे मलेशियाले कुनै औपचारिक जबाफ दिएको छैन।'],
          ...
        ]

        '''

        # .............................................
        print(f' columns : {chunk[0]}')

        # First row
        url = chunk[1][0]
        title = chunk[1][1]
        paragraphs = chunk[1][2]
        print(f' row-1: url:{url},  title:{title}, \n paragraphs: {paragraphs}')

        # do processing stuff
        break

# Decode unicode example
# not necessary right now
import re

def decode_unicode_escapes(s):
    try:
      # Pattern to match Unicode escape sequences
      unicode_pattern = re.compile(r'\\u[0-9a-fA-F]{4}')

      # Decode only the parts of the string that match the Unicode pattern
      return unicode_pattern.sub(lambda m: m.group(0).encode().decode('unicode_escape'), s)
    except UnicodeDecodeError:
        # In case of an error, return the original string
        return s
    except Exception as e:
        print(f"An error occurred: {e}")
        return s  # Return the original string in case of an error

# Example strings
data1 = "\\u0938\\u0941\\u0930\\u094d\\u0916\\u0947\\u0924 \\u0930 \\u091c\\u0941\\u092e"
data2 = "नेपालीपोष्ट \\u0928– Nepali news site."

# Decoding
decoded_data1 = decode_unicode_escapes(data1)
decoded_data2 = decode_unicode_escapes(data2)

print(decoded_data1)  # Outputs: सुरक्षित र जुम
print(decoded_data2)  # Outputs: नेपालीपोष्ट – Nepali news site.

chunk[2]