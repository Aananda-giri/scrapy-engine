{"cells":[{"cell_type":"markdown","metadata":{"id":"UMyxOS05TzoW"},"source":["## # **Code to merge newly-crawled data and previously-crawled data**"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":628,"status":"ok","timestamp":1723173808715,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"9Bm1oMcwVNUS","outputId":"7323b04e-62f4-489d-9463-826bb55948b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Research/datasets/crawled_data\n"]}],"source":["%cd /content/drive/MyDrive/Research/datasets/crawled_data\n","# %cd /content/drive/MyDrive/Research/datasets\n","# %cd /content/drive/MyDrive/Research/datasets/crawled_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGs7FpYQTpYd"},"outputs":[],"source":["!pip install boto3 -q\n","\n","# Upload data to ec2\n","!python3 s3_v2.py drive/MyDrive/Research/datasets/crawled_data/crawl-processed/urls.db"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31839,"status":"ok","timestamp":1723173736177,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"Lt57uRkgtoHI","outputId":"e82a878f-4e6c-4a96-d233-85b071a43adb"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-08-09 03:21:45--  https://1b-bucket.s3.ap-southeast-2.amazonaws.com/crawled_data.csv?AWSAccessKeyId=AKIAYS2NTSLAOGT5XYEF&Signature=egVpjYKobSpYqTQYX%2BV3eGTpZzg%3D&Expires=1723346472\n","Resolving 1b-bucket.s3.ap-southeast-2.amazonaws.com (1b-bucket.s3.ap-southeast-2.amazonaws.com)... 52.95.132.238, 52.95.134.54, 52.95.131.50, ...\n","Connecting to 1b-bucket.s3.ap-southeast-2.amazonaws.com (1b-bucket.s3.ap-southeast-2.amazonaws.com)|52.95.132.238|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 623511250 (595M) [binary/octet-stream]\n","Saving to: ‘crawled_data_new.csv’\n","\n","crawled_data_new.cs 100%[===================>] 594.63M  20.7MB/s    in 30s     \n","\n","2024-08-09 03:22:16 (20.0 MB/s) - ‘crawled_data_new.csv’ saved [623511250/623511250]\n","\n"]}],"source":["!wget \"https://1b-bucket.s3.ap-southeast-2.amazonaws.com/crawled_data.csv?AWSAccessKeyId=AKIAYS2NTSLAOGT5XYEF&Signature=egVpjYKobSpYqTQYX%2BV3eGTpZzg%3D&Expires=1723346472\" -O \"crawled_data_new.csv\""]},{"cell_type":"markdown","metadata":{"id":"HG7PO-wszTWn"},"source":["## Merge Two csv files"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":86513,"status":"ok","timestamp":1723175176300,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"_X940f2WJRVL","outputId":"668ae6b4-47e8-4eed-ccb4-ba3b034036d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," completed old_data file.\n","\n","\n"," removed id of new_data file.\n","merge complete!\n","Merged CSV file has been created at merged_crawled_data.csv\n"]}],"source":["import csv\n","\n","old_file_path = 'crawled_data.csv'\n","new_file_path = 'crawled_data_new.csv'\n","output_file_path = 'merged_crawled_data.csv'\n","\n","def clean_rows(row, source=None):\n","    # If the old row has four columns, remove the first column\n","    if len(row) == 4:\n","        return row[1:]\n","        if source == 1:\n","            # Old data is not supposed to contain four columns lets see if they do.\n","            print(f'four columns: {row}')\n","    elif len(row) == 3:\n","        return row\n","    else:\n","        print(f\"Unexpected number of columns: {len(row)} row: {row}\")\n","    return row\n","\n","def merge_csv_files(old_file_path, new_file_path, output_file_path):\n","    with open(old_file_path, 'r', newline='', encoding='utf-8') as old_file, \\\n","         open(new_file_path, 'r', newline='', encoding='utf-8') as new_file, \\\n","         open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n","\n","        old_reader = csv.reader(old_file)\n","        new_reader = csv.reader(new_file)\n","        writer = csv.writer(output_file)\n","\n","        # Write the headers from old file\n","        old_headers = ['parent_url', 'page_title', 'paragraph']\n","        writer.writerow(old_headers)\n","\n","        # Skip the header in the old file and write cleaned old data\n","        next(old_reader)\n","        for row in old_reader:\n","            cleaned_row = clean_rows(row, source=1)\n","            writer.writerow(cleaned_row)\n","\n","        print(f'\\n\\n completed iterating through old_data file: {old_file_path}')\n","\n","        # Skip the header in the new file and write new data with necessary fields\n","        next(new_reader)\n","        for row in new_reader:\n","            writer.writerow(clean_rows(row, source=2))\n","        print(f'\\n\\n removed id of new_data file: {new_file_path}')\n","    print('merge complete!')\n","# Run the merging function\n","merge_csv_files(old_file_path, new_file_path, output_file_path)\n","\n","print(f\"Merged CSV file has been created at {output_file_path}\")\n"]},{"cell_type":"markdown","source":["# Migration\n","* current_headers: ['parent_url', 'page_title', 'paragraph']\n","* new_headers: ['parent_url', 'page_title', 'paragraphs']\n","\n","Where, paragraphs is list of paragraph from current_headers\n","\n","# Handles:\n","* Duplicate crawled urls\n","* duplocate paragraphs of same page"],"metadata":{"id":"sS4e8pITRcY4"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108626,"status":"ok","timestamp":1723175319814,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"Stj0zq9nJYmF","outputId":"79524427-4bdf-44db-b81d-632a7609edef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Consolidated CSV file has been created at consolidated_crawled_data.csv\n"]}],"source":["import csv\n","from collections import defaultdict\n","import json\n","\n","input_file_path = 'merged_crawled_data.csv'\n","output_file_path = 'consolidated_crawled_data.csv'\n","\n","def consolidate_paragraphs(input_file_path, output_file_path):\n","    '''\n","    # old-format:\n","      {\n","        <url-11>:{'page_title': 'page-1', 'paragraph': 'devanagari-paragraph-1'},\n","        <url-1>:{'page_title': page-1' 'paragraph': 'devanagari-paragraph-2'},\n","        <url-2>:{'page_title': page-2, 'paragraph': 'devanagari-paragraph-3'}\n","      }\n","    # new-format:\n","      {\n","        <url-11>:{'page_title': 'page-1', 'paragraphs': ['devanagari-paragraph-1', 'devanagari-paragraph-2']},\n","        <url-2>:{'page_title': page-2, 'paragraphs': ['devanagari-paragraph-2', 'devanagari-paragraph-3']}\n","      }\n","\n","      * paragraphs are jsonified list i.e. saved after performing `json.dumps(paragraphs)`\n","    '''\n","\n","    # Using list for paragraphs because set dont preserve order.\n","    data = defaultdict(lambda: {'page_title': None, 'paragraphs': []})\n","\n","    # Read the input CSV file\n","    with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file:\n","        reader = csv.DictReader(input_file)\n","\n","        for row in reader:\n","            if 'paragraph' in row.keys():\n","                # previous data format used to have `paragraph` as string\n","                # New data format have `paragraphs` as list\n","                # print(f'\\n\\n old data format. Not supposed to be in newer batch of')\n","                parent_url = row['parent_url']\n","                page_title = row['page_title']\n","                paragraph = row['paragraph']\n","\n","                if parent_url not in data:\n","                    # Add new data-item\n","                    data[parent_url]['page_title'] = page_title\n","\n","                if paragraph not in data[parent_url]['paragraphs']:\n","                    # append paragraph\n","                    # set would give better time complexity but they dont preserve order.\n","                    data[parent_url]['paragraphs'].append(paragraph)\n","            elif 'paragraphs' in row.keys():\n","                # Newer data format\n","                parent_url = row['parent_url']\n","                page_title = row['page_title']\n","                paragraphs = row['paragraphs']\n","\n","                if parent_url not in data:\n","                    # Add new data-item\n","                    data[parent_url]['page_title'] = page_title\n","                data[parent_url]['paragraphs'].extend(json.loads(paragraphs)) # Extend paragraphs\n","\n","    # Write the consolidated data to the output CSV file\n","    with open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n","        fieldnames = ['parent_url', 'page_title', 'paragraphs']\n","        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n","\n","        writer.writeheader()\n","        for parent_url, content in data.items():\n","            writer.writerow({\n","                'parent_url': parent_url,\n","                'page_title': content['page_title'],\n","                'paragraphs': json.dumps(content['paragraphs'])  # Convert set to list before dumping to JSON\n","            })\n","\n","# Run the consolidation function\n","consolidate_paragraphs(input_file_path, output_file_path)\n","\n","print(f\"Consolidated CSV file has been created at {output_file_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1723001281740,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"BjoYqiEm3Zd6","outputId":"4bfda5d9-adc4-47c2-cd00-a3b2ee362b81"},"outputs":[{"name":"stdout","output_type":"stream","text":["3\n"]}],"source":["from itertools import islice\n","\n","with open(old_file_path, 'r') as my_file:\n","    reader = csv.reader(my_file)\n","    next_line = next(reader)\n","    while next_line:\n","      (len(next_line))\n","          print(next_line)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":557,"status":"ok","timestamp":1723001208342,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"aur1flX02Uz8","outputId":"bab68a4a-4d31-4562-b448-8ee2368a948d"},"outputs":[{"data":{"text/plain":["<_csv.reader at 0x7bacf2d1d540>"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["reader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOaSfHdG2KDX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaxGkkSfi52N"},"outputs":[],"source":["import os\n","import json\n","\n","def remove_file_if_empty(file_path):\n","  \"\"\"Checks if a file is empty and removes it if it is.\n","\n","    Args:\n","        file_path (str): The path to the file to check.\n","\n","    Returns:\n","        bool: True if the file was empty and removed, False otherwise.\n","    \"\"\"\n","  if os.path.exists(file_path):\n","    if os.path.getsize(file_path) == 0:\n","      # File size is 0 bytes\n","      try:\n","        os.remove(file_path)\n","        print(f\"Removed empty file: {file_path}\")\n","        return True\n","      except OSError as e:\n","        print(f\"Error removing file: {e}\")\n","        return False\n","    else:\n","      try:\n","        with open(file_path, 'r') as f:\n","          data = json.load(f)\n","      except Exception as Ex:\n","        print(f'----------------------------------- Exception: {Ex} -----------------------------------\\n  file_path: {file_path}\\n ')\n","        with open(file_path, 'a') as f:\n","          f.write(\"\\\"\\\"]\")\n","        with open(file_path, 'r') as f:\n","          data = json.load(f)\n","\n","      if not data:\n","        # File is empty\n","        try:\n","          os.remove(file_path)\n","          print(f\"Removed empty file: {file_path}\")\n","          return True\n","        except OSError as e:\n","          print(f\"Error removing file: {e}\")\n","          return False\n","  else:\n","    print(f\"File is not empty or does not exist: {file_path}\")\n","    return False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":83454,"status":"error","timestamp":1710141052738,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"__SD7AXQLASy","outputId":"a7c2acc7-d07b-41e2-cb60-898aca794761"},"outputs":[{"name":"stdout","output_type":"stream","text":["0/40 file: hamrokhotang.com.json\n","1/40 file: nepalihimal.com.json\n","2/40 file: onlinetvnepal.com.json\n","3/40 file: baahrakhari.com.json\n","4/40 file: radiosagarmatha.org.np.json\n","5/40 file: realkhabar.net.json\n","6/40 file: abhiyandaily.com.json\n","7/40 file: bigulnews.com.json\n","8/40 file: www.nagariknetwork.com.json\n","9/40 file: saptahik.com.np.json\n","10/40 file: kantipath.com.json\n","11/40 file: www.nayapage.com.json\n","12/40 file: www.onsnews.com.json\n","13/40 file: ekantipur.com.json\n","14/40 file: aarthiknews.com_merged_.json\n","15/40 file: samacharpati.com_merged_.json\n","16/40 file: sancharkendra.com_merged_.json\n","17/40 file: www.bizshala.com_merged_.json\n","18/40 file: www.eadarsha.com_merged_.json\n","19/40 file: www.onlinekhabar.com_merged_.json\n","20/40 file: hamrakura.com.json\n","21/40 file: www.bizshala.com.json\n","22/40 file: www.bizshala.com_5.json\n","23/40 file: www.bizshala.com_2.json\n","24/40 file: www.bizshala.com_3.json\n","25/40 file: www.arghakhanchi.com.json\n","26/40 file: gorkhapatraonline.com.json\n","27/40 file: nepalipost.com.json\n","28/40 file: topnepalnews.com_0.json\n","29/40 file: topnepalnews.com_1.json\n","30/40 file: www.newsofnepal.com.json\n","31/40 file: _merged1_.json\n","32/40 file: www.dainiknepal.com.json\n","33/40 file: samudrapari.com.json\n","34/40 file: samudrapari.com_0.json\n","35/40 file: _merged_0.json\n","37/40 file: nepalsamaya.com_0.json\n","38/40 file: www.newsofnepal.com_0.json\n"]},{"ename":"JSONDecodeError","evalue":"Expecting value: line 1061908 column 1 (char 180965544)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-f281ff617002>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All valid files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1061908 column 1 (char 180965544)"]}],"source":["# --------------------------------------\n","# Test if all files are json readable\n","# --------------------------------------\n","import os, json\n","data_files = [\n","      file for file in os.listdir() if (file.endswith('.json')) and (\n","          file not in\n","          ['test.json', 'news_start_urls copy.json', 'news_start_urls.json'])\n","  ]\n","for n, file in enumerate(data_files):\n","    print(f\"{n}/{len(data_files)} file: {file}\")\n","    with open(file,'r') as f:\n","      data = json.load(f)\n","print(\"All valid files\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EXbDc4URWin0","outputId":"6f306189-8ed2-4d39-a2f5-0d232382ce74"},"outputs":[{"name":"stderr","output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}],"source":["with open(\"www.newsofnepal.com_0.json\",'r') as f:\n","  data=f.read()\n","data[-10:]\n","new = data.replace('\\\"\\\"]','')\n","print(new[-10:])\n","# with open(\"www.newsofnepal.com_0.json\",'w') as f:\n","#   data=f.write()\n","\n","# with open(\"www.newsofnepal.com_0.json\", 'a') as f:\n","#         f.write(\"]\")\n","\n","# with open(\"www.newsofnepal.com_0.json\",'r') as f:\n","#   data=f.readlines()\n","# data[-2:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11225,"status":"ok","timestamp":1710143943340,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"aZsyhoBqWVfh","outputId":"c233122c-d5e1-4229-da2e-73baf5a42f58"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","2\n","3\n","4\n","Expecting ',' delimiter: line 2 column 40 (char 41)\n","6\n","7\n","9\n","Expecting ',' delimiter: line 2 column 40 (char 41)\n"]}],"source":["file=\"arthasarokar.com.json\"\n","file=\"www.newsofnepal.com_0.json\"\n","try:\n","    with open(file, 'r', encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","except Exception as e:\n","    try:\n","        print(1)\n","        with open(file, \"r\", encoding=\"utf-8\") as f:\n","            # Read the entire content of the file\n","            content = f.read()\n","        print(2)\n","        # Replace all occurrences of the EN DASH character with a hyphen\n","        new_content = content.replace(\"–\", \"-\")\n","        print(3)\n","        # Open the output file in write mode\n","        with open(file, \"w\", encoding=\"utf-8\") as f:\n","            # Write the modified content to the output file\n","            f.write(new_content)\n","        print(4)\n","        with open(file, 'r', encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        print(5)\n","    except Exception as e:\n","        print(e)\n","        try:\n","            print(6)\n","            with open(file,'a') as f:\n","                f.write(\"\\\"\\\"]\")\n","            print(7)\n","            with open(file, 'r', encoding=\"utf-8\") as f:\n","                data = json.load(f)\n","            print(8)\n","        except Exception as ex:\n","            print(9)\n","            print(ex)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":2294,"status":"ok","timestamp":1710143902993,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"9Io8wvUUgl4U","outputId":"04f63fc7-05d6-47d3-c2c0-fbed44fb1ca7"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Example\"}]'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["import json\n","with open(file, \"r\", encoding=\"utf-8\") as f:\n","    # Read the entire content of the file\n","    # content = json.load(f)\n","    content=f.read()\n","\n","content[-10:]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4324,"status":"ok","timestamp":1710143849120,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"kdWLVPM2gDvv","outputId":"f6665652-7076-4da5-db14-75c20d6a57ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n","3\n"]}],"source":["with open(file, \"r\", encoding=\"utf-8\") as f:\n","    # Read the entire content of the file\n","    content = f.read()\n","\n","print(2)\n","# Replace all occurrences of the EN DASH character with a hyphen\n","new_content = content.replace(',\\n','')\n","\n","print(3)\n","# Open the output file in write mode\n","with open(file, \"w\", encoding=\"utf-8\") as f:\n","    # Write the modified content to the output file\n","    f.write(new_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1710143345360,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"QQ0d69ZlfFY9","outputId":"48e09eb5-589d-4d97-d9d1-547f0550b96e"},"outputs":[{"data":{"text/plain":["[{'parent_url': 'https://arthasarokar.com/tag/archive/page/1184',\n","  'url': 'https://www.youtube.com/arthasarokar',\n","  'text': '\\n\\n',\n","  'is_social_media': True,\n","  'social_media_type': 'youtube'},\n"," {'parent_url': 'https://arthasarokar.com/tag/archive/page/1184',\n","  'url': 'https://www.tiktok.com/@arthasarokar',\n","  'text': '\\n\\n',\n","  'is_social_media': True,\n","  'social_media_type': 'tiktok'},\n"," {'visited': 'https://arthasarokar.com/tag/archive/page/1185'},\n"," {'parent_url': 'https://arthasarokar.com/tag/archive/page/1185',\n","  'page_title': \"ARCHIVE – Page 1185 – Artha Sarokar :: Nepal's No.1 Economic News portal.\",\n","  'paragraph': ' बासुकी मार्ग, कोटेश्वर-काठमाडौँ\\n',\n","  'is_nepali_confidence': -196.83209705352783},\n"," '']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["data[-5:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"executionInfo":{"elapsed":2793,"status":"error","timestamp":1710143927886,"user":{"displayName":"Aananda Giri","userId":"10248715758347187876"},"user_tz":-345},"id":"8BN6eBBjeYm5","outputId":"2bc85b17-4079-41e0-9923-140a727dc62b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Removed crawled_nepali_news_dataset.csv\n","\n","0/40: hamrokhotang.com.json : 0Mb\n","\n","\n","1/40: nepalihimal.com.json : 0.36214Mb\n","\n","\n","2/40: onlinetvnepal.com.json : 1.060297Mb\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-88ceb8928e20>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0msave_nepali_paragraphs_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-39-88ceb8928e20>\u001b[0m in \u001b[0;36msave_nepali_paragraphs_to_csv\u001b[0;34m(csv_file_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n{n}/{len(data_files)}: {file} : {os.path.getsize(csv_file_name)/1000000 if os.path.exists(csv_file_name) else 0}Mb\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mremove_file_if_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;31m# Removed empty file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-d0919387ad57>\u001b[0m in \u001b[0;36mremove_file_if_empty\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mEx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'----------------------------------- Exception: {Ex} -----------------------------------\\n  file_path: {file_path}\\n '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import csv\n","import pandas as pd\n","def save_nepali_paragraphs_to_csv(csv_file_name = \"crawled_nepali_news_dataset.csv\"):\n","  '''\n","    '''\n","  # remove file if it exists\n","  if os.path.exists(csv_file_name):\n","    print(f'Removed {csv_file_name}')\n","    os.remove(csv_file_name)\n","  data_files = [\n","      file for file in os.listdir() if (file.endswith('.json')) and (\n","          file not in\n","          ['test.json', 'news_start_urls copy.json', 'news_start_urls.json'])\n","  ]\n","  for n, file in enumerate(data_files):\n","    print(f'\\n{n}/{len(data_files)}: {file} : {os.path.getsize(csv_file_name)/1000000 if os.path.exists(csv_file_name) else 0}Mb\\n')\n","    if remove_file_if_empty(file):\n","      continue\n","      # Removed empty file\n","    try:\n","      # load data from each file\n","      with open(file, 'r') as f:\n","        data = json.load(f)\n","    except:\n","      '''\n","               file is corrupt when scrapy is terminated while it is still crawling.\n","               while corrupt, file is terminated with: `{some_data},`\n","               adding : `\"\"]` at the end of file to make it valid json file\n","\n","      '''\n","      with open(file, 'a') as f:\n","        f.write(\"\\\"\\\"]\")\n","      with open(file, 'r') as f:\n","        data = json.load(f)\n","    nepali_paragraphs = []\n","    for d in data:\n","      if type(d) == dict:\n","        if 'paragraph' in d.keys():\n","          nepali_paragraphs.append(d)\n","          #   print(list(d.values()))\n","          #   # save dataset in a csv file\n","          #   with open(csv_file_name, 'a', newline='') as csv_file:\n","          #     # Create a CSV writer object\n","          #     csv_writer = csv.writer(csv_file)\n","          #     # Write the data to the CSV file\n","          #     csv_writer.writerows(list(d.values()))\n","          # print(f'len_nepali_paragraphs:{len(nepali_paragraphs)} keys:{nepali_paragraphs[0].keys()}')\n","    # Open the CSV file in append mode\n","    with open(csv_file_name, 'a', newline='') as csv_file:\n","        # Create a CSV writer object\n","        csv_writer = csv.writer(csv_file)\n","\n","        # Check if the CSV file is empty\n","        is_empty = os.path.getsize(csv_file_name) == 0\n","\n","        # Extract unique column names from dictionary keys\n","        column_names = set(key for d in nepali_paragraphs for key in d.keys())\n","\n","        # Write the header only if the CSV file is empty\n","        if is_empty:\n","            csv_writer.writerow(column_names)\n","\n","        # Iterate through each dictionary and write rows to CSV\n","        for d in nepali_paragraphs:\n","            # Create a list of values for the row, using empty string for missing keys\n","            row_values = [str(d.get(column, '')) for column in column_names]\n","            csv_writer.writerow(row_values)\n","    # ----------------------------\n","    # Drop duplicate rows in csv\n","    # ----------------------------\n","\n","    # Read the CSV file into a DataFrame\n","    df = pd.read_csv(csv_file_name)\n","    # Drop duplicate rows\n","    df.drop_duplicates(inplace=True)\n","    # Write the cleaned DataFrame back to the CSV file\n","    df.to_csv(csv_file_name, index=False)\n","\n","os.remove\n","save_nepali_paragraphs_to_csv()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y41o38PPe4ql"},"outputs":[],"source":["!ls"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1nx0Vs-EsD59Ss_Py8l-evVL6tslTGLiI","authorship_tag":"ABX9TyP4wgaXog76X/LjwHLDUbsK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}