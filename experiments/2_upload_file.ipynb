{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'182ccedb33a9e03fbf1079b209da1a31_20250111_105205_564156_temp.pickle'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "class PickleUtils:\n",
    "    @staticmethod\n",
    "    def _generate_url_hash(url: str) -> str:\n",
    "        \"\"\"Generate MD5 hash of URL.\"\"\"\n",
    "        return hashlib.md5(url.encode()).hexdigest()\n",
    "    # print(generate_url_hash('https://example.com/'))    # 182ccedb33a9e03fbf1079b209da1a31\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_file_name(url: str) -> str:\n",
    "        \"\"\"\n",
    "        Create filename with hash and timestamp.\n",
    "        \n",
    "        url_hash: MD5 hash of URL\n",
    "        timestamp: current timestamp\n",
    "        filename: {url_hash}_{timestamp}_temp.pickle\n",
    "        \"\"\"\n",
    "        url_hash = PickleUtils._generate_url_hash(url)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        suffix = \"_temp.pickle\"\n",
    "        return f\"{url_hash}_{timestamp}{suffix}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_html(url, html_content):\n",
    "        '''\n",
    "            * save html data in filename: hash(url) + _temp.pickle\n",
    "            * move _temp.pickle -> pickles/<filename>.pickle\n",
    "        '''\n",
    "        # 1) get filename\n",
    "        filename = PickleUtils._get_file_name(url)\n",
    "        \n",
    "        # 2) save data to <filename>_temp.pickle\n",
    "        data = {'url':url, 'html_content':html_content}\n",
    "        # save to pickle file\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        \n",
    "        os.makedirs('pickles', exist_ok=True)\n",
    "        \n",
    "        # 3) rename <filename>_temp.pickle to <filename>.pickle\n",
    "        os.rename(filename, filename.replace('_temp', ''))\n",
    "    \n",
    "    def load_pickle(filename=None):\n",
    "        if not filename:\n",
    "            filename=\"fa5b40e417c5cb81fb5c31d6ba6903da_20250110_212913_581403.pickle\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pickle_utils = PickleUtils\n",
    "    PickleUtils._get_file_name('https://example.com/')   # '182ccedb33a9e03fbf1079b209da1a31_20250107_152420_350083_temp.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to upload zip files to s3/hf periodically\n",
    "\n",
    "### todo : \n",
    "* make sure os.listdir lists pickle files while crawling (i.e. pickle files path is correctly provided)\n",
    "* set HF_TOKEN in environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import uuid\n",
    "import zipfile\n",
    "\n",
    "\n",
    "from s3_v2 import Ec2Functions\n",
    "from huggingface_hub import HfApi\n",
    "from python_dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def zip_pickles(pickle_dir: str = \"./\") -> str:    # self\n",
    "        \"\"\"\n",
    "        * Zip pickle files using UUID as filename.\n",
    "        * Delete raw files that were zipped\n",
    "        \"\"\"\n",
    "        zip_filename = f\"{uuid.uuid4()}.zip\"\n",
    "        \n",
    "        pickle_path = Path(pickle_dir)\n",
    "            \n",
    "        # Get all non-temp pickle files\n",
    "        pickle_files = [\n",
    "            f for f in pickle_path.glob(\"*.pickle\")\n",
    "            if not f.name.endswith(\"_temp.pickle\")\n",
    "        ]\n",
    "        \n",
    "        # 1) Zip .pickle files\n",
    "        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            \n",
    "            print(f'pickle files: {pickle_files}')\n",
    "            # Add files to zip\n",
    "            for file in pickle_files:\n",
    "                zipf.write(file, file.name)\n",
    "            \n",
    "            # Add manifest\n",
    "            manifest = f\"\"\"Backup created on: {datetime.now()}\n",
    "                        Total files: {len(pickle_files)}\n",
    "                        Files included:\n",
    "                        {chr(10).join(f'- {f.name}' for f in pickle_files)}\n",
    "                        \"\"\"\n",
    "            zipf.writestr(\"manifest.txt\", manifest)\n",
    "        \n",
    "        # 2) Delete raw files that were zipped\n",
    "        for file_name in pickle_files:\n",
    "            os.remove(file_name)\n",
    "        \n",
    "        return zip_filename\n",
    "\n",
    "def upload_zip_to_s3(zip_filename:str):    # self\n",
    "    try:\n",
    "        # 1) upload zip file to s3\n",
    "        Ec2Functions.upload_file(file_path=zip_filename, bucket_name='1b-bucket', object_key=zip_filename)\n",
    "\n",
    "        # 2) remove local zip file\n",
    "        os.remove(zip_filename)\n",
    "    except Exception as ex:\n",
    "        print('failed to upload to s3', ex)\n",
    "\n",
    "api = HfApi()\n",
    "def upload_zip_to_hf(zip_filename:str):\n",
    "    try:\n",
    "        # 1) upload zip file to huggingface\n",
    "        api.upload_file(\n",
    "        path_or_fileobj=zip_filename,\n",
    "        path_in_repo=f'scrapy_engine/raw_chunks/{zip_filename}',\n",
    "        repo_id=\"Aananda-giri/nepali_llm_datasets\",\n",
    "        repo_type=\"dataset\",\n",
    "        token=token\n",
    "        )\n",
    "\n",
    "        # 2) remove local zip file\n",
    "        os.remove(zip_filename)\n",
    "        \n",
    "        return True # success\n",
    "    except Exception as ex:\n",
    "        print('failed to upload to hf', ex)\n",
    "        return False\n",
    "\n",
    "def get_pickles_size(pickle_dir: str = \"./\"):\n",
    "    '''\n",
    "        * sum of size of all .pickle files in Mb\n",
    "    '''\n",
    "    pickle_path = Path(pickle_dir)\n",
    "    pickle_files = [\n",
    "            f for f in pickle_path.glob(\"*.pickle\")\n",
    "            if not f.name.endswith(\"_temp.pickle\")\n",
    "        ]\n",
    "    \n",
    "    # Assuming 'pathlib.Path' object is used for pickle_path\n",
    "    total_size = sum(f.stat().st_size / (1024 * 1024) for f in pickle_files)\n",
    "    return total_size\n",
    "\n",
    "\n",
    "# pseudocode\n",
    "'''\n",
    "* zip and upload:\n",
    "    once every hour \n",
    "    or if pickles_size > 100MB  (check once every 5 minutes )\n",
    "'''\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    time_elapsed = time.time() - start_time\n",
    "    \n",
    "    pickle_files_size_mb = get_pickles_size()\n",
    "    if time_elapsed > 3600 or pickle_files_size_mb > 100:\n",
    "        # 3600 seconds in one hour\n",
    "        start_time = time.time()    # reset start_time\n",
    "        \n",
    "        zip_filename = zip_pickles()\n",
    "        uploaded_to_hf = upload_zip_to_hf(zip_filename)\n",
    "        if not uploaded_to_hf:\n",
    "            upload_zip_to_s3(zip_filename)\n",
    "        \n",
    "        # delete zip file\n",
    "        os.remove(zip_filename)\n",
    "    \n",
    "    # sleep for 5 minutes\n",
    "    time.speep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def __main__(self):\n",
    "    '''\n",
    "        main of worker spider in scrapy\n",
    "    '''\n",
    "    background_service = BackgroundUploadService()\n",
    "    background_upload_thread = threading.Thread(target = background_service.run())\n",
    "    background_upload_thread.daemon = True\n",
    "    background_upload_thread.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
