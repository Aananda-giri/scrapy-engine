{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save_other_data\n",
    "*  boolean variable stored in server.py which is updated in online_mongo\n",
    "* spider checks this variable in online_mongo to decide whether or not to crawl other_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crawl_other_data': False, 'crawl_paragraph_data': True}\n"
     ]
    }
   ],
   "source": [
    "# Save to mongo\n",
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# # unique Index for config name\n",
    "# mongo.db['config'].create_index('name', unique=True)\n",
    "\n",
    "# # Remove index\n",
    "# mongo.db['config'].drop_index('name_1')\n",
    "\n",
    "configs = [\n",
    "        {'crawl_other_data': False},\n",
    "        {'crawl_paragraph_data':True},\n",
    "        # {'some_config':1000}\n",
    "    ]\n",
    "# Save configs\n",
    "mongo.set_configs(configs)\n",
    "\n",
    "print('Configs saved')\n",
    "\n",
    "# Get configs\n",
    "configs = mongo.get_configs()\n",
    "crawled_data = got_configs['crawl_other_data'] in got_configs[0]\n",
    " and got_configs[0]['crawl_other_data'] == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl_other_data: False\n",
      "crawl_paragraph_data: True\n"
     ]
    }
   ],
   "source": [
    "# Get configs\n",
    "configs = mongo.get_configs()\n",
    "# default: Do not Crawl other data\n",
    "crawl_other_data = configs['crawl_other_data'] if 'crawl_other_data' in configs else False\n",
    "print(f'crawl_other_data: {crawl_other_data}')\n",
    "# default: Crawl paragraph data\n",
    "crawl_paragraph_data = configs['crawl_paragraph_data'] if 'crawl_paragraph_data' in configs else True\n",
    "print(f'crawl_paragraph_data: {crawl_paragraph_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'crawl_other_data' in configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "crawl_other_data = mongo.db['config'].find_one({'name': 'save_other_data'})\n",
    "if crawl_other_data:\n",
    "            print(crawl_other_data['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_other_data': False, 'some_config': 1000, 'crawl_paragraph_data': True, 'crawl_other_data': False}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('665c2b374e0b765a6d0780aa'),\n",
       "  'name': 'save_other_data',\n",
       "  'value': False}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = list(mongo.db['config'].find({}))\n",
    "[config for config in configs if config['name'] == 'save_other_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mongo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# list(mongo.db['config'].find({}))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmongo\u001b[49m\u001b[38;5;241m.\u001b[39mdb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdelete_many({})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mongo' is not defined"
     ]
    }
   ],
   "source": [
    "# list(mongo.db['config'].find({}))\n",
    "mongo.db['config'].delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mongoDb  is full:\n",
    "```\n",
    " * more than 900k urls to_crawl\n",
    " * >160K crawled_urls\n",
    " * (500MB) Mongo free\n",
    "```\n",
    "#### Solution:\n",
    "* only keep 100k-200K to_crawls in mongo at a time\n",
    "* remove crawled_urls\n",
    "* Avoid crawling data urls\n",
    "\n",
    "get all to_crawl (920054)   | Save to csv\n",
    "get all crawled  (160842)   | Save to csv\n",
    "\n",
    "\n",
    "\n",
    "* make scrapy_engine_spider to push to \"to_crawl?\" instead of \"to_crawl\"\n",
    "* avoid updating \"crawled\" from scrapy_spider to mongo\n",
    "\n",
    "* Get Urls from \"to_crawl?\"\n",
    "    [ ] Store Crawled urls in bloom function.\n",
    "    [ ] bloom filter to check urls in \"to_crawl?\" does not exist in urls_crawled\n",
    "    [X] store to csv/sqlite (avoid duplicate): csv files: \"to_crawl\", \"crawled\" or sqlite_tables\n",
    "    [X] Delete to_crawls and crawled from mongo\n",
    "\n",
    "    if len(to_crawl_in_mongo < 100000):\n",
    "        [ ] shuffle(to_crawl_urls)\n",
    "        [X] add 100k to_crawl urls from local to mongo\n",
    "        [X] delete all attempted mongo inserts.\n",
    "        [ ] delete only successful mongo insert and store unsuccessful ones somewhere\n",
    "\n",
    "update to_crawl and \"crawled\" using crawled_data\n",
    "[X] for data in crawled_data:\n",
    "        if data['parent_url'] in to_crawl:\n",
    "            remove data['parent_url'] from to_crawl\n",
    "        append_to_crawled(data['parent_url'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note:\n",
    "* \"to_crawl?\" -> uploaded by scrapy spider to mongo\n",
    "* \"to_crawl\"  -> actual to_crawl link uploaded to mongo by server\n",
    "* sqlite seems to handle concurrency by itself without throwing errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Error Data from Mongo and save it to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from mongo import Mongo\n",
    "mongo=Mongo()\n",
    "\n",
    "error_data = mongo.collection.find({'status': 'error'}) # .limit(10)\n",
    "formatted_for_csv = [{\n",
    "        'url': error['url'],\n",
    "        'timestamp': error['timestamp'],\n",
    "        'status': error['status'],\n",
    "        'status_code': error['status_code'] if 'status_code' in error else None,\n",
    "        'error_type': error['error_type']\n",
    "    } for error in error_data]\n",
    "# Append error data to csv\n",
    "csv_file_path = 'error_data.csv'\n",
    "file_exists = os.path.exists(csv_file_path)\n",
    "with open(csv_file_path, 'a') as csvfile:\n",
    "    fieldnames = ['url', 'timestamp', 'status', 'status_code', 'error_type']\n",
    "    csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    if not file_exists:\n",
    "        print(f\"creating csv file: {csv_file_path}\")\n",
    "        # Write header only if file is empty\n",
    "        csv_writer.writeheader()\n",
    "    else:\n",
    "        print(f'csv file: \\\"{csv_file_path}\\\" exists')\n",
    "    csv_writer.writerows(formatted_for_csv)\n",
    "\n",
    "print(f'Saved error_data to {csv_file_path}')\n",
    "\n",
    "# Delete from mongo\n",
    "mongo.collection.delete_many({'url': {'$in': [error['url'] for error in formatted_for_csv]}, 'status': 'error'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_for_csv=[{'url': 'https://www.dainiknepal.com/', 'timestamp': 1714996985.424532, 'status': 'error', 'status_code': 403, 'error_type': 'HttpError'}, {'url': 'https://nepalkhabar.com/', 'timestamp': 1714996985.8666382, 'status': 'error', 'status_code': 403, 'error_type': 'HttpError'}, {'url': 'https://belkotgadhimun.gov.np/faq', 'timestamp': 1715018151.0237563, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://epsnepal.gov.np/documents/service-sector-%e0%a4%ae%e0%a4%be-%e0%a4%b0%e0%a5%8b%e0%a4%b7%e0%a5%8d%e0%a4%9f%e0%a4%b0-%e0%a4%aa%e0%a4%b0%e0%a4%bf%e0%a4%b5%e0%a4%b0%e0%a5%8d%e0%a4%a4%e0%a4%a8-%e0%a4%b8%e0%a4%ae%e0%a5%8d%e0%a4%b5/', 'timestamp': 1715036967.8365157, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://www.bbc.com/nepali/send/u50853473', 'timestamp': 1715078773.2192092, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://www.bbc.com/nepali/undefined', 'timestamp': 1715083194.0109007, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://www.bbc.com/nepali/resources/%5Bhttps:/www.bbc.com/nepali%5D', 'timestamp': 1715092717.9155746, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'http://dohs.gov.np/ne/mohpnep', 'timestamp': 1715097008.6633568, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'http://nhtc.gov.np/index.php/trainingevent/training', 'timestamp': 1715098146.3797266, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://mofaga.gov.np/prov-1', 'timestamp': 1715100428.9299655, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.dainiknepal.com/',\n",
       " 'https://nepalkhabar.com/',\n",
       " 'https://belkotgadhimun.gov.np/faq',\n",
       " 'https://epsnepal.gov.np/documents/service-sector-%e0%a4%ae%e0%a4%be-%e0%a4%b0%e0%a5%8b%e0%a4%b7%e0%a5%8d%e0%a4%9f%e0%a4%b0-%e0%a4%aa%e0%a4%b0%e0%a4%bf%e0%a4%b5%e0%a4%b0%e0%a5%8d%e0%a4%a4%e0%a4%a8-%e0%a4%b8%e0%a4%ae%e0%a5%8d%e0%a4%b5/',\n",
       " 'https://www.bbc.com/nepali/send/u50853473',\n",
       " 'https://www.bbc.com/nepali/undefined',\n",
       " 'https://www.bbc.com/nepali/resources/%5Bhttps:/www.bbc.com/nepali%5D',\n",
       " 'http://dohs.gov.np/ne/mohpnep',\n",
       " 'http://nhtc.gov.np/index.php/trainingevent/training',\n",
       " 'https://mofaga.gov.np/prov-1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# Get 10 data urls from to_crawl.\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'}).limit(10)\n",
    "\n",
    "# get urls starting with 'data:'\n",
    "data_urls = mongo.collection.find({\"url\": {\"$regex\": \"^data:\"}})\n",
    "list(data_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "920054"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongo.collection.count_documents({\"status\": 'to_crawl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Cursor' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m db \u001b[38;5;241m=\u001b[39m Mongo()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count Entries with status=\"to_crawl\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto_crawl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Cursor' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "db = Mongo()\n",
    "# Count Entries with status=\"to_crawl\"\n",
    "db.collection.find({\"status\": \"to_crawl\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Time-Operation: Get all urls from mongo and save to sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "# get all to_crawl urls 50000 at a time\n",
    "# Initialize skip to 0\n",
    "skip = 0\n",
    "bulk_size = 50000\n",
    "\n",
    "the_to_crawl_urls = []\n",
    "while True:\n",
    "    # Get the next 50000 documents\n",
    "    to_crawl_urls = mongo.collection.find({\"status\": 'to_crawl'}).skip(skip).limit(bulk_size)\n",
    "    # Convert the cursor to a list\n",
    "    crawled_urls_list = list(to_crawl_urls)\n",
    "    # If the list is empty, break the loop\n",
    "    if not crawled_urls_list:\n",
    "        break\n",
    "    # Process the documents\n",
    "    the_to_crawl_urls.extend([(url['url'], url['timestamp']) for url in crawled_urls_list])\n",
    "    # Increase skip by 50000\n",
    "    skip += bulk_size\n",
    "    print(skip)\n",
    "\n",
    "len(the_to_crawl_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"to_crawl\", the_to_crawl_urls)\n",
    "\n",
    "url_db.count_entries(\"to_crawl\")\n",
    "\n",
    "# # Get all the urls from the database\n",
    "# urls = url_db.fetch('to_crawl', 10)\n",
    "# urls\n",
    "\n",
    "# delete from mongo\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "# get all crawled urls 50000 at a time\n",
    "# Initialize skip to 0\n",
    "skip = 0\n",
    "bulk_size = 50000\n",
    "\n",
    "the_crawled_urls = []\n",
    "while True:\n",
    "    # Get the next 50000 documents\n",
    "    crawled_urls = mongo.collection.find({\"status\": 'crawled'}).skip(skip).limit(bulk_size)\n",
    "    # Convert the cursor to a list\n",
    "    crawled_urls_list = list(crawled_urls)\n",
    "    # If the list is empty, break the loop\n",
    "    if not crawled_urls_list:\n",
    "        break\n",
    "    # Process the documents\n",
    "    the_crawled_urls.extend([(url['url'], url['timestamp']) for url in crawled_urls_list])\n",
    "    # Increase skip by 50000\n",
    "    skip += bulk_size\n",
    "    print(skip)\n",
    "\n",
    "len(the_crawled_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", the_crawled_urls)\n",
    "\n",
    "\n",
    "import time;start=time.time();a=url_db.fetch('crawled', 100000);print(start-time.time())\n",
    "\n",
    "# # Get all the urls from the database\n",
    "urls = url_db.fetch('crawled', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 3, 'electionId': ObjectId('7fffffff0000000000000405'), 'opTime': {'ts': Timestamp(1716833546, 33), 't': 1029}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716833546, 35), 'signature': {'hash': b\"\\xa7.\\x84\\xd8K\\x97\\xd6\\xbc\\xf5\\x08W'\\x06\\xe1o\\x9b\\xa0\\xaf\\x1cl\", 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1716833546, 33)}, acknowledged=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "\n",
    "sample_url = {'url': 'https://wdww.iana.org/f_img/2013.1/iana-logo-header.svg1', 'status': 'to_crawl?', 'timestamp': '2021-07-01T00:00:00.000Z'}\n",
    "mongo.collection.insert_one(sample_url)\n",
    "\n",
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "to_crawl_urls\n",
    "\n",
    "# Remove multiple url from \"to_crawl?\"\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl?', \"url\": {\"$in\": [url[0] for url in to_crawl_urls]}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_crawl_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m url_db \u001b[38;5;241m=\u001b[39m URLDatabase(db_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murls.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# url_db.exists(\"crawled\", to_crawl_urls[0][0])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Insert the data into the database\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m url_db\u001b[38;5;241m.\u001b[39mbulk_insert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrawled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mto_crawl_urls\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Delete \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# url_db.count_entries(\"crawled\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# # Get all the urls from the database\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# # urls = url_db.fetch_all('test')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_crawl_urls' is not defined"
     ]
    }
   ],
   "source": [
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "to_crawl_urls\n",
    "\n",
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# url_db.exists(\"crawled\", to_crawl_urls[0][0])\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", to_crawl_urls)\n",
    "\n",
    "# Delete \n",
    "# url_db.count_entries(\"crawled\")\n",
    "\n",
    "# url_db.fetch_all(\"crawled\")\n",
    "\n",
    "# # Get all the urls from the database\n",
    "# # urls = url_db.fetch_all('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "* creating a giant thread to avoid concurrency issues\n",
    "'''\n",
    "import sys\n",
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "\n",
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "# to_crawl_urls\n",
    "\n",
    "# Save to sqlite\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", to_crawl_urls)\n",
    "\n",
    "# delete from mongo\n",
    "mongo.collection.delete_many([{'url': url[0], 'status': 'to_crawl?'} for url in to_crawl_urls])\n",
    "\n",
    "\n",
    "if mongo.collection.count_documents({\"status\": 'to_crawl'}) < 100000:\n",
    "    new_to_crawl_urls = url_db.fetch('to_crawl', 100000)\n",
    "    n_failed_to_upload = 0\n",
    "    try:\n",
    "        # insert many\n",
    "        mongo.collection.insert_many([{'url': url[0], 'status': 'to_crawl', 'timestamp': url[1]} for url in new_to_crawl_urls], ordered=False)\n",
    "    except Exception as bwe:\n",
    "        # pass\n",
    "        # Get the details of the operations that failed\n",
    "        failed_ops = bwe.details['writeErrors']\n",
    "        \n",
    "\n",
    "        # Get the documents that failed to insert\n",
    "        failed_docs = [op['op'] for op in failed_ops]\n",
    "\n",
    "        # Get the URLs that failed to insert\n",
    "        urls_failed_to_upload_to_mongo = [(doc['url'], doc['timestamp']) for doc in failed_docs]\n",
    "        n_failed_to_upload = len(urls_failed_to_upload_to_mongo)\n",
    "        # for url in urls_failed_to_upload_to_mongo:\n",
    "        #     logging.error(f\"Failed to upload {url} to MongoDB\")\n",
    "        # success_urls = [url for url in new_to_crawl_urls if url not in urls_failed_to_upload_to_mongo]\n",
    "\n",
    "        # # Delete successful urls from sqlite\n",
    "        # url_db.delete(\"to_crawl\", success_urls)\n",
    "        # # print(f'success_urls:{success_urls}, len:{len(success_urls)}')\n",
    "        # print(failed_urls, len(failed_urls))\n",
    "    # # delete from sqlite\n",
    "    if n_failed_to_upload < 10000:\n",
    "        url_db.delete(\"to_crawl\", new_to_crawl_urls)\n",
    "        # print(n_failed_to_upload)\n",
    "    else:\n",
    "        print(f'failed to upload {n_failed_to_upload} urls to mongo')\n",
    "        # exit the python script\n",
    "        sys.exit(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 0, 'electionId': ObjectId('7fffffff0000000000000405'), 'opTime': {'ts': Timestamp(1716835666, 11), 't': 1029}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716835666, 11), 'signature': {'hash': b'a\\x0cbO\\xfd\"\\xda\\xff0,*\\xbaK&\\xad@\\xba\\xb5\\xe6o', 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1716835666, 11)}, acknowledged=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# Delete all urls with status 'crawled'\n",
    "# mongo.collection.delete_many({\"status\": 'to_crawl'})\n",
    "mongo.collection.delete_many({\"status\": 'crawled'})\n",
    "mongo.collection.delete_many({\"status\": 'test'})\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# crawled_count = mongo.collection.count_documents({\"status\": 'crawled'})\n",
    "to_crawl_spider_count = mongo.collection.count_documents({\"status\": 'to_crawl?'})\n",
    "to_crawl_spider_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102914\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "crawled_data_count = mongo.collection.count_documents({'status':'crawling'})\n",
    "\n",
    "other_data_count = mongo.db['other_data'].count_documents({})\n",
    "print(crawled_data_count)\n",
    "print(other_data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Before 5 minutes\n",
    "timestamp = time.time()\n",
    "\n",
    "mongo.collection.count_documents({'status':'crawling', 'timestamp': ['$lt', '1714996990.47649']})\n",
    "# mongo.collection.count_documents({'status':'crawling', { $gt: [\"$timeStamp\", 1432201420790] }})\n",
    "\n",
    "# using aggregate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: recover_expired_crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Before 5 minutes\n",
    "timestamp = time.time()\n",
    "pipeline = [\n",
    "    {\"$match\": {\"status\": \"crawling\", \"timestamp\": {\"$lt\": str(timestamp)}}},\n",
    "    # {\"$count\": \"count\"}\n",
    "]\n",
    "# The result is a list of documents returned by the aggregation pipeline\n",
    "expired_crawling_urls = list(mongo.collection.aggregate(pipeline))\n",
    "\n",
    "expired_crawling_urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_crawling_to_to_crawl(urls):\n",
    "            # for url in urls:\n",
    "            #     self.collection.update_one(\n",
    "            #         {'_id':url['_id'], 'status': {'$in': ['crawling']}},\n",
    "            #         {'$set': {'status':'to_crawl'}}\n",
    "            #         )\n",
    "            # perform bulk update\n",
    "            if urls:\n",
    "                mongo.collection.update_many(\n",
    "                    {'_id': {'$in': [url['_id'] for url in urls]}},\n",
    "                    {'$set': {'status':'to_crawl'}}\n",
    "                )\n",
    "\n",
    "\n",
    "convert_from_crawling_to_to_crawl(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' count to_crawl'\n",
    "mongo.collection.count_documents({'status':'to_crawl?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo is full by indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo=Mongo()\n",
    "\n",
    "mongo.check_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 1, 'electionId': ObjectId('7fffffff000000000000000e'), 'opTime': {'ts': Timestamp(1716896107, 4000), 't': 14}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716896107, 4000), 'signature': {'hash': b'\\xc0\\x84\\x0e\\x19\\x11\\xa6\\xae\\x92\\xcd\\x85\\x8e\\xfb\\n\\xf1.h\\x8f\\x8d\\xe1E', 'keyId': 7351058967755227142}}, 'operationTime': Timestamp(1716896107, 4000)}, acknowledged=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a url to 'to_crawl' status\n",
    "import time\n",
    "# mongo.collection.insert_one({'url': 'https://www.bbc.com/nepali', 'status': 'to_crawl', 'timestamp': time.time()})\n",
    "# mongo.collection.create_index('url', unique=True)\n",
    "mongo.collection.delete_many({'status': 'to_crawl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all collections\n",
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "a = mongo.recover_expired_crawling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expired_crawling_urls=a\n",
    "# expired_crawling_urls[0]['url']\n",
    "from sqlite_handler import URLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sqlite\n",
    "another_sqlite_instance = URLDatabase(db_path=\"urls.db\")\n",
    "another_sqlite_instance.fetch_all(\"to_crawl\")\n",
    "not_already_crawled = []\n",
    "for entry in expired_crawling_urls:\n",
    "    if not another_sqlite_instance.exists(\"crawled\", entry['url']):\n",
    "        not_already_crawled.append(entry)\n",
    "# Save to sqlite\n",
    "# entries = ['url': url['url'], 'timestamp': url['timestamp']} for url in not_already_crawled]\n",
    "entries2 = [(url['url'], url['timestamp']) for url in not_already_crawled]\n",
    "if entries2:\n",
    "    another_sqlite_instance.bulk_insert(\"to_crawl\", entries2, show_progress=False)\n",
    "# another_sqlite_instance.close()\n",
    "\n",
    "# Delete from mongo\n",
    "mongo.collection.delete_many({'status': 'crawling', 'url': {'$in': [entry['url'] for entry in not_already_crawled]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_sqlite_instance.bulk_insert(\"to_crawl\", entries, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_sqlite_instance.count_entries('to_crawl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
