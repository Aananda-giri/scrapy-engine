{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mongoDb  is full:\n",
    "```\n",
    " * more than 900k urls to_crawl\n",
    " * >160K crawled_urls\n",
    " * (500MB) Mongo free\n",
    "```\n",
    "#### Solution:\n",
    "* only keep 100k-200K to_crawls in mongo at a time\n",
    "* remove crawled_urls\n",
    "* Avoid crawling data urls\n",
    "\n",
    "get all to_crawl (920054)   | Save to csv\n",
    "get all crawled  (160842)   | Save to csv\n",
    "\n",
    "\n",
    "\n",
    "* make scrapy_engine_spider to push to \"to_crawl?\" instead of \"to_crawl\"\n",
    "* avoid updating \"crawled\" from scrapy_spider to mongo\n",
    "\n",
    "* Get Urls from \"to_crawl?\"\n",
    "    [ ] Store Crawled urls in bloom function.\n",
    "    [ ] bloom filter to check urls in \"to_crawl?\" does not exist in urls_crawled\n",
    "    [X] store to csv/sqlite (avoid duplicate): csv files: \"to_crawl\", \"crawled\" or sqlite_tables\n",
    "    [X] Delete to_crawls and crawled from mongo\n",
    "\n",
    "    if len(to_crawl_in_mongo < 100000):\n",
    "        [ ] shuffle(to_crawl_urls)\n",
    "        [X] add 100k to_crawl urls from local to mongo\n",
    "        [X] delete all attempted mongo inserts.\n",
    "        [ ] delete only successful mongo insert and store unsuccessful ones somewhere\n",
    "\n",
    "update to_crawl and \"crawled\" using crawled_data\n",
    "[X] for data in crawled_data:\n",
    "        if data['parent_url'] in to_crawl:\n",
    "            remove data['parent_url'] from to_crawl\n",
    "        append_to_crawled(data['parent_url'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note:\n",
    "* \"to_crawl?\" -> uploaded by scrapy spider to mongo\n",
    "* \"to_crawl\"  -> actual to_crawl link uploaded to mongo by server\n",
    "* sqlite seems to handle concurrency by itself without throwing errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Error Data from Mongo and save it to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from mongo import Mongo\n",
    "mongo=Mongo()\n",
    "\n",
    "error_data = mongo.collection.find({'status': 'error'}) # .limit(10)\n",
    "formatted_for_csv = [{\n",
    "        'url': error['url'],\n",
    "        'timestamp': error['timestamp'],\n",
    "        'status': error['status'],\n",
    "        'status_code': error['status_code'] if 'status_code' in error else None,\n",
    "        'error_type': error['error_type']\n",
    "    } for error in error_data]\n",
    "# Append error data to csv\n",
    "csv_file_path = 'error_data.csv'\n",
    "file_exists = os.path.exists(csv_file_path)\n",
    "with open(csv_file_path, 'a') as csvfile:\n",
    "    fieldnames = ['url', 'timestamp', 'status', 'status_code', 'error_type']\n",
    "    csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    if not file_exists:\n",
    "        print(f\"creating csv file: {csv_file_path}\")\n",
    "        # Write header only if file is empty\n",
    "        csv_writer.writeheader()\n",
    "    else:\n",
    "        print(f'csv file: \\\"{csv_file_path}\\\" exists')\n",
    "    csv_writer.writerows(formatted_for_csv)\n",
    "\n",
    "print(f'Saved error_data to {csv_file_path}')\n",
    "\n",
    "# Delete from mongo\n",
    "mongo.collection.delete_many({'url': {'$in': [error['url'] for error in formatted_for_csv]}, 'status': 'error'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_for_csv=[{'url': 'https://www.dainiknepal.com/', 'timestamp': 1714996985.424532, 'status': 'error', 'status_code': 403, 'error_type': 'HttpError'}, {'url': 'https://nepalkhabar.com/', 'timestamp': 1714996985.8666382, 'status': 'error', 'status_code': 403, 'error_type': 'HttpError'}, {'url': 'https://belkotgadhimun.gov.np/faq', 'timestamp': 1715018151.0237563, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://epsnepal.gov.np/documents/service-sector-%e0%a4%ae%e0%a4%be-%e0%a4%b0%e0%a5%8b%e0%a4%b7%e0%a5%8d%e0%a4%9f%e0%a4%b0-%e0%a4%aa%e0%a4%b0%e0%a4%bf%e0%a4%b5%e0%a4%b0%e0%a5%8d%e0%a4%a4%e0%a4%a8-%e0%a4%b8%e0%a4%ae%e0%a5%8d%e0%a4%b5/', 'timestamp': 1715036967.8365157, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://www.bbc.com/nepali/send/u50853473', 'timestamp': 1715078773.2192092, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://www.bbc.com/nepali/undefined', 'timestamp': 1715083194.0109007, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://www.bbc.com/nepali/resources/%5Bhttps:/www.bbc.com/nepali%5D', 'timestamp': 1715092717.9155746, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'http://dohs.gov.np/ne/mohpnep', 'timestamp': 1715097008.6633568, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'http://nhtc.gov.np/index.php/trainingevent/training', 'timestamp': 1715098146.3797266, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://mofaga.gov.np/prov-1', 'timestamp': 1715100428.9299655, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.dainiknepal.com/',\n",
       " 'https://nepalkhabar.com/',\n",
       " 'https://belkotgadhimun.gov.np/faq',\n",
       " 'https://epsnepal.gov.np/documents/service-sector-%e0%a4%ae%e0%a4%be-%e0%a4%b0%e0%a5%8b%e0%a4%b7%e0%a5%8d%e0%a4%9f%e0%a4%b0-%e0%a4%aa%e0%a4%b0%e0%a4%bf%e0%a4%b5%e0%a4%b0%e0%a5%8d%e0%a4%a4%e0%a4%a8-%e0%a4%b8%e0%a4%ae%e0%a5%8d%e0%a4%b5/',\n",
       " 'https://www.bbc.com/nepali/send/u50853473',\n",
       " 'https://www.bbc.com/nepali/undefined',\n",
       " 'https://www.bbc.com/nepali/resources/%5Bhttps:/www.bbc.com/nepali%5D',\n",
       " 'http://dohs.gov.np/ne/mohpnep',\n",
       " 'http://nhtc.gov.np/index.php/trainingevent/training',\n",
       " 'https://mofaga.gov.np/prov-1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# Get 10 data urls from to_crawl.\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'}).limit(10)\n",
    "\n",
    "# get urls starting with 'data:'\n",
    "data_urls = mongo.collection.find({\"url\": {\"$regex\": \"^data:\"}})\n",
    "list(data_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "920054"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongo.collection.count_documents({\"status\": 'to_crawl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Cursor' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m db \u001b[38;5;241m=\u001b[39m Mongo()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count Entries with status=\"to_crawl\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto_crawl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Cursor' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "db = Mongo()\n",
    "# Count Entries with status=\"to_crawl\"\n",
    "db.collection.find({\"status\": \"to_crawl\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Time-Operation: Get all urls from mongo and save to sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "# get all to_crawl urls 50000 at a time\n",
    "# Initialize skip to 0\n",
    "skip = 0\n",
    "bulk_size = 50000\n",
    "\n",
    "the_to_crawl_urls = []\n",
    "while True:\n",
    "    # Get the next 50000 documents\n",
    "    to_crawl_urls = mongo.collection.find({\"status\": 'to_crawl'}).skip(skip).limit(bulk_size)\n",
    "    # Convert the cursor to a list\n",
    "    crawled_urls_list = list(to_crawl_urls)\n",
    "    # If the list is empty, break the loop\n",
    "    if not crawled_urls_list:\n",
    "        break\n",
    "    # Process the documents\n",
    "    the_to_crawl_urls.extend([(url['url'], url['timestamp']) for url in crawled_urls_list])\n",
    "    # Increase skip by 50000\n",
    "    skip += bulk_size\n",
    "    print(skip)\n",
    "\n",
    "len(the_to_crawl_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"to_crawl\", the_to_crawl_urls)\n",
    "\n",
    "url_db.count_entries(\"to_crawl\")\n",
    "\n",
    "# # Get all the urls from the database\n",
    "# urls = url_db.fetch('to_crawl', 10)\n",
    "# urls\n",
    "\n",
    "# delete from mongo\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "# get all crawled urls 50000 at a time\n",
    "# Initialize skip to 0\n",
    "skip = 0\n",
    "bulk_size = 50000\n",
    "\n",
    "the_crawled_urls = []\n",
    "while True:\n",
    "    # Get the next 50000 documents\n",
    "    crawled_urls = mongo.collection.find({\"status\": 'crawled'}).skip(skip).limit(bulk_size)\n",
    "    # Convert the cursor to a list\n",
    "    crawled_urls_list = list(crawled_urls)\n",
    "    # If the list is empty, break the loop\n",
    "    if not crawled_urls_list:\n",
    "        break\n",
    "    # Process the documents\n",
    "    the_crawled_urls.extend([(url['url'], url['timestamp']) for url in crawled_urls_list])\n",
    "    # Increase skip by 50000\n",
    "    skip += bulk_size\n",
    "    print(skip)\n",
    "\n",
    "len(the_crawled_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", the_crawled_urls)\n",
    "\n",
    "\n",
    "import time;start=time.time();a=url_db.fetch('crawled', 100000);print(start-time.time())\n",
    "\n",
    "# # Get all the urls from the database\n",
    "urls = url_db.fetch('crawled', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 3, 'electionId': ObjectId('7fffffff0000000000000405'), 'opTime': {'ts': Timestamp(1716833546, 33), 't': 1029}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716833546, 35), 'signature': {'hash': b\"\\xa7.\\x84\\xd8K\\x97\\xd6\\xbc\\xf5\\x08W'\\x06\\xe1o\\x9b\\xa0\\xaf\\x1cl\", 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1716833546, 33)}, acknowledged=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "\n",
    "sample_url = {'url': 'https://wdww.iana.org/f_img/2013.1/iana-logo-header.svg1', 'status': 'to_crawl?', 'timestamp': '2021-07-01T00:00:00.000Z'}\n",
    "mongo.collection.insert_one(sample_url)\n",
    "\n",
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "to_crawl_urls\n",
    "\n",
    "# Remove multiple url from \"to_crawl?\"\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl?', \"url\": {\"$in\": [url[0] for url in to_crawl_urls]}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_crawl_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m url_db \u001b[38;5;241m=\u001b[39m URLDatabase(db_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murls.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# url_db.exists(\"crawled\", to_crawl_urls[0][0])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Insert the data into the database\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m url_db\u001b[38;5;241m.\u001b[39mbulk_insert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrawled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mto_crawl_urls\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Delete \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# url_db.count_entries(\"crawled\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# # Get all the urls from the database\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# # urls = url_db.fetch_all('test')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_crawl_urls' is not defined"
     ]
    }
   ],
   "source": [
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "to_crawl_urls\n",
    "\n",
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# url_db.exists(\"crawled\", to_crawl_urls[0][0])\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", to_crawl_urls)\n",
    "\n",
    "# Delete \n",
    "# url_db.count_entries(\"crawled\")\n",
    "\n",
    "# url_db.fetch_all(\"crawled\")\n",
    "\n",
    "# # Get all the urls from the database\n",
    "# # urls = url_db.fetch_all('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "* creating a giant thread to avoid concurrency issues\n",
    "'''\n",
    "import sys\n",
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "\n",
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "# to_crawl_urls\n",
    "\n",
    "# Save to sqlite\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", to_crawl_urls)\n",
    "\n",
    "# delete from mongo\n",
    "mongo.collection.delete_many([{'url': url[0], 'status': 'to_crawl?'} for url in to_crawl_urls])\n",
    "\n",
    "\n",
    "if mongo.collection.count_documents({\"status\": 'to_crawl'}) < 100000:\n",
    "    new_to_crawl_urls = url_db.fetch('to_crawl', 100000)\n",
    "    n_failed_to_upload = 0\n",
    "    try:\n",
    "        # insert many\n",
    "        mongo.collection.insert_many([{'url': url[0], 'status': 'to_crawl', 'timestamp': url[1]} for url in new_to_crawl_urls], ordered=False)\n",
    "    except Exception as bwe:\n",
    "        # pass\n",
    "        # Get the details of the operations that failed\n",
    "        failed_ops = bwe.details['writeErrors']\n",
    "        \n",
    "\n",
    "        # Get the documents that failed to insert\n",
    "        failed_docs = [op['op'] for op in failed_ops]\n",
    "\n",
    "        # Get the URLs that failed to insert\n",
    "        urls_failed_to_upload_to_mongo = [(doc['url'], doc['timestamp']) for doc in failed_docs]\n",
    "        n_failed_to_upload = len(urls_failed_to_upload_to_mongo)\n",
    "        # for url in urls_failed_to_upload_to_mongo:\n",
    "        #     logging.error(f\"Failed to upload {url} to MongoDB\")\n",
    "        # success_urls = [url for url in new_to_crawl_urls if url not in urls_failed_to_upload_to_mongo]\n",
    "\n",
    "        # # Delete successful urls from sqlite\n",
    "        # url_db.delete(\"to_crawl\", success_urls)\n",
    "        # # print(f'success_urls:{success_urls}, len:{len(success_urls)}')\n",
    "        # print(failed_urls, len(failed_urls))\n",
    "    # # delete from sqlite\n",
    "    if n_failed_to_upload < 10000:\n",
    "        url_db.delete(\"to_crawl\", new_to_crawl_urls)\n",
    "        # print(n_failed_to_upload)\n",
    "    else:\n",
    "        print(f'failed to upload {n_failed_to_upload} urls to mongo')\n",
    "        # exit the python script\n",
    "        sys.exit(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 0, 'electionId': ObjectId('7fffffff0000000000000405'), 'opTime': {'ts': Timestamp(1716835666, 11), 't': 1029}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716835666, 11), 'signature': {'hash': b'a\\x0cbO\\xfd\"\\xda\\xff0,*\\xbaK&\\xad@\\xba\\xb5\\xe6o', 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1716835666, 11)}, acknowledged=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# Delete all urls with status 'crawled'\n",
    "# mongo.collection.delete_many({\"status\": 'to_crawl'})\n",
    "mongo.collection.delete_many({\"status\": 'crawled'})\n",
    "mongo.collection.delete_many({\"status\": 'test'})\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# crawled_count = mongo.collection.count_documents({\"status\": 'crawled'})\n",
    "to_crawl_spider_count = mongo.collection.count_documents({\"status\": 'to_crawl?'})\n",
    "to_crawl_spider_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102914\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "crawled_data_count = mongo.collection.count_documents({'status':'crawling'})\n",
    "\n",
    "other_data_count = mongo.db['other_data'].count_documents({})\n",
    "print(crawled_data_count)\n",
    "print(other_data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Before 5 minutes\n",
    "timestamp = time.time()\n",
    "\n",
    "mongo.collection.count_documents({'status':'crawling', 'timestamp': ['$lt', '1714996990.47649']})\n",
    "# mongo.collection.count_documents({'status':'crawling', { $gt: [\"$timeStamp\", 1432201420790] }})\n",
    "\n",
    "# using aggregate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: recover_expired_crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Before 5 minutes\n",
    "timestamp = time.time()\n",
    "pipeline = [\n",
    "    {\"$match\": {\"status\": \"crawling\", \"timestamp\": {\"$lt\": str(timestamp)}}},\n",
    "    # {\"$count\": \"count\"}\n",
    "]\n",
    "# The result is a list of documents returned by the aggregation pipeline\n",
    "expired_crawling_urls = list(mongo.collection.aggregate(pipeline))\n",
    "\n",
    "expired_crawling_urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_crawling_to_to_crawl(urls):\n",
    "            # for url in urls:\n",
    "            #     self.collection.update_one(\n",
    "            #         {'_id':url['_id'], 'status': {'$in': ['crawling']}},\n",
    "            #         {'$set': {'status':'to_crawl'}}\n",
    "            #         )\n",
    "            # perform bulk update\n",
    "            if urls:\n",
    "                mongo.collection.update_many(\n",
    "                    {'_id': {'$in': [url['_id'] for url in urls]}},\n",
    "                    {'$set': {'status':'to_crawl'}}\n",
    "                )\n",
    "\n",
    "\n",
    "convert_from_crawling_to_to_crawl(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' count to_crawl'\n",
    "mongo.collection.count_documents({'status':'to_crawl?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo is full by indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationFailure",
     "evalue": "CMD_NOT_ALLOWED: compact, full error: {'ok': 0, 'errmsg': 'CMD_NOT_ALLOWED: compact', 'code': 8000, 'codeName': 'AtlasError'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationFailure\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m mongo\u001b[38;5;241m=\u001b[39mMongo()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compacting the database\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# mongo.collection.reindex()  # reindex the collection\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmongo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompact\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murls-collection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/_csot.py:108\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/database.py:897\u001b[0m, in \u001b[0;36mDatabase.command\u001b[0;34m(self, command, value, check, allowable_errors, read_preference, codec_options, session, comment, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m     read_preference \u001b[38;5;241m=\u001b[39m (session \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_txn_read_preference()) \u001b[38;5;129;01mor\u001b[39;00m ReadPreference\u001b[38;5;241m.\u001b[39mPRIMARY\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__client\u001b[38;5;241m.\u001b[39m_conn_for_reads(read_preference, session, operation\u001b[38;5;241m=\u001b[39mcommand_name) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[1;32m    894\u001b[0m     connection,\n\u001b[1;32m    895\u001b[0m     read_preference,\n\u001b[1;32m    896\u001b[0m ):\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/database.py:737\u001b[0m, in \u001b[0;36mDatabase._command\u001b[0;34m(self, conn, command, value, check, allowable_errors, read_preference, codec_options, write_concern, parse_write_concern_error, session, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m command\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__client\u001b[38;5;241m.\u001b[39m_tmp_session(session) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m--> 737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/helpers.py:327\u001b[0m, in \u001b[0;36m_handle_reauth.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Connection\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailure \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_reauth:\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/pool.py:985\u001b[0m, in \u001b[0;36mConnection.command\u001b[0;34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_not_writable(unacknowledged)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_mongos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlisteners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bson_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_op_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_msg_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43munacknowledged\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munacknowledged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexhaust_allowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexhaust_allowed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (OperationFailure, NotPrimaryError):\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/network.py:212\u001b[0m, in \u001b[0;36mcommand\u001b[0;34m(conn, dbname, spec, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields, exhaust_allowed, write_concern)\u001b[0m\n\u001b[1;32m    210\u001b[0m             client\u001b[38;5;241m.\u001b[39m_process_response(response_doc, session)\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m check:\n\u001b[0;32m--> 212\u001b[0m             \u001b[43mhelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_command_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresponse_doc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_wire_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    219\u001b[0m     duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/helpers.py:233\u001b[0m, in \u001b[0;36m_check_command_response\u001b[0;34m(response, max_wire_version, allowable_errors, parse_write_concern_error)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m43\u001b[39m:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CursorNotFound(errmsg, code, response, max_wire_version)\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OperationFailure(errmsg, code, response, max_wire_version)\n",
      "\u001b[0;31mOperationFailure\u001b[0m: CMD_NOT_ALLOWED: compact, full error: {'ok': 0, 'errmsg': 'CMD_NOT_ALLOWED: compact', 'code': 8000, 'codeName': 'AtlasError'}"
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo=Mongo()\n",
    "\n",
    "mongo.collection.count_documents({'status':'crawling'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
