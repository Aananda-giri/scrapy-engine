{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/mnt/resources2/weekly-projects/scrapy_engine_v3/server/crawled_data/output/redirect_links/redirect_links_d4ffb8bb0c5915b218014856d301b209_20250112_161251_191255.pickle'),\n",
       " PosixPath('/mnt/resources2/weekly-projects/scrapy_engine_v3/server/crawled_data/output/redirect_links/redirect_links_270eea41476d716ba2660f23dd1d1280_20250112_161253_292935.pickle')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"./output\")\n",
    "redirect_links_dir = Path(output_dir / \"redirect_links\").resolve()  # Resolve to absolute path\n",
    "# \"\"\"Get all non-temporary pickle files in the directory.\"\"\"\n",
    "        \n",
    "        \n",
    "pickle_files = [\n",
    "    f for f in redirect_links_dir.glob(\"*.pickle\")\n",
    "    if not f.name.endswith(\"_temp.pickle\")\n",
    "]\n",
    "\n",
    "pickle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\t\t Creaetd new bloom filter\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "\t\t Creaetd new bloom filter\n",
      "============================================================\n",
      "\n",
      "to_crawl:71\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from bloom import get_bloom_thread\n",
    "\n",
    "crawled_bloom = get_bloom_thread(save_file='crawled_bloom_filter.pkl', scalable=True)\n",
    "error_bloom = get_bloom_thread(save_file='error_bloom_filter.pkl', scalable=True)\n",
    "\n",
    "# all_links=set()\n",
    "links_to_crawl = []\n",
    "\n",
    "for pickle_file in pickle_files:\n",
    "    redirect_links = []\n",
    "    with open(pickle_file,'rb') as file:\n",
    "        redirect_links = pickle.load(file)\n",
    "        \n",
    "    for link in redirect_links:\n",
    "        # all_links.add(link)\n",
    "        if link not in crawled_bloom and link not in error_bloom:\n",
    "            crawled_bloom.add(link)\n",
    "            error_bloom.add(link)\n",
    "            links_to_crawl.append(link)\n",
    "\n",
    "print(f'to_crawl:{len(links_to_crawl)}')\n",
    "# print(f'all:{len(all_links)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Bloom filter saved to file: crawled_bloom_filter.pkl\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Bloom filter saved to file: error_bloom_filter.pkl\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crawled_bloom.save()\n",
    "error_bloom.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_urls_from_pickle(self):\n",
    "    \"\"\"\n",
    "    Load redirect URLs from pickle files into MongoDB after filtering through bloom filters.\n",
    "    \n",
    "    The function:\n",
    "    1. Reads redirect URLs from pickle files in the redirect_links directory\n",
    "    2. Filters URLs using bloom filters to avoid duplicates\n",
    "    3. Adds new URLs to MongoDB with metadata\n",
    "    4. Updates and saves bloom filters\n",
    "    5. Cleans up processed pickle files\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of new URLs added to MongoDB\n",
    "    \"\"\"\n",
    "    # Get all non-temporary pickle files\n",
    "    pickle_files = list(self.redirect_links_dir.glob(\"[!_]*[!_temp].pickle\"))\n",
    "    \n",
    "    if not pickle_files:\n",
    "        self.logger.info(\"No pickle files found to process\")\n",
    "        return 0\n",
    "    \n",
    "    links_to_crawl = set()  # Using set for better performance\n",
    "    \n",
    "    try:\n",
    "        # Process each pickle file\n",
    "        for pickle_path in pickle_files:\n",
    "            try:\n",
    "                with pickle_path.open('rb') as file:\n",
    "                    redirect_links = pickle.load(file)\n",
    "                    \n",
    "                    # Filter links through bloom filters\n",
    "                    new_links = {\n",
    "                        link for link in redirect_links\n",
    "                        if link not in self.crawled_bloom \n",
    "                        and link not in self.error_bloom\n",
    "                    }\n",
    "                    \n",
    "                    # Update bloom filters with new links\n",
    "                    for link in new_links:\n",
    "                        self.crawled_bloom.add(link)\n",
    "                        self.error_bloom.add(link)\n",
    "                    \n",
    "                    links_to_crawl.update(new_links)\n",
    "                    \n",
    "            except (IOError, pickle.UnpicklingError) as e:\n",
    "                self.logger.error(f\"Failed to process {pickle_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Prepare documents for MongoDB\n",
    "        if links_to_crawl:\n",
    "            current_time = time.time()\n",
    "            docs = [\n",
    "                {\n",
    "                    \"url\": link,\n",
    "                    \"status\": \"to_crawl\",\n",
    "                    \"timestamp\": current_time,\n",
    "                    \"crawling_count\": 0\n",
    "                } for link in links_to_crawl\n",
    "            ]\n",
    "            \n",
    "            # Batch insert into MongoDB\n",
    "            try:\n",
    "                result = self.collection.insert_many(docs, ordered=False)\n",
    "                inserted_count = len(result.inserted_ids)\n",
    "                self.logger.info(f\"Added {inserted_count} new URLs to MongoDB\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to insert documents to MongoDB: {str(e)}\")\n",
    "                # Still save bloom filters even if MongoDB insert fails\n",
    "                raise\n",
    "            \n",
    "            # Save bloom filters after successful MongoDB insertion\n",
    "            try:\n",
    "                self.crawled_bloom.save()\n",
    "                self.error_bloom.save()\n",
    "                self.logger.info(\"Bloom filters updated and saved\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to save bloom filters: {str(e)}\")\n",
    "                raise\n",
    "        \n",
    "        # Clean up processed pickle files\n",
    "        for pickle_path in pickle_files:\n",
    "            try:\n",
    "                pickle_path.unlink()\n",
    "            except OSError as e:\n",
    "                self.logger.error(f\"Failed to remove {pickle_path}: {str(e)}\")\n",
    "        \n",
    "        return len(links_to_crawl)\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Unexpected error in load_urls_from_pickle: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd', 'e', 'f'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=set()\n",
    "[a.add(i) for i in ['a','b','c']]\n",
    "\n",
    "b=['d','e','f']\n",
    "a.update(b)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
