{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import redis\n",
    "import shutil\n",
    "import threading\n",
    "\n",
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "local_mongo = Mongo(local=True)\n",
    "\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, filename=\"log.log\", filemode=\"w\", format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "'''\n",
    "logging.debug(\"debug\")       # logs all the logs below\n",
    "logging.info(\"info\")         # log all except debug log\n",
    "logging.warning(\"warning\")  # log warning, error, critical      (Default)\n",
    "logging.error(\"error\")      # log error, critical\n",
    "logging.critical(\"critical\")    # log critical\n",
    "\n",
    "try:\n",
    "    1/0\n",
    "except Exception as ex:\n",
    "    logging.exception(ex)\n",
    "'''\n",
    "\n",
    "# ======================================================\n",
    "# Crawling  ==> to_crawl (if timestamp >2 hours)\n",
    "# ======================================================\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "# number_of_links_crawled_at_start = mongo.collection.count_documents({\"status\": \"crawled\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revoke_crawling_url():\n",
    "    '''\n",
    "    * Get crawling urls from mongo\n",
    "    * if they are not in crawled in local_mongo:\n",
    "        * update status in local_mongo, status=tocrawl\n",
    "        * delete them from mongo\n",
    "    \n",
    "    local_mongo\n",
    "        `collection.update_many({\"url\": {\"$in\": urls}, \"status\": \"crawling\"}, {\"$set\": {\"status\": \"to_crawl\"}}, upsert=True)`\n",
    "        status:crawling <change it to to_crawl>\n",
    "        status:crawled <leave as it is>\n",
    "        status:to_crawl <leave as it is>\n",
    "    \n",
    "    # Tests:\n",
    "    import time\n",
    "    from mongo import Mongo\n",
    "    local_mongo = Mongo(local=True)\n",
    "    local_mongo.collection.insert_many([{'url': 'url_crawling', 'status': 'crawling', 'timestamp': time.time()}, {'url': 'url_crawled', 'status': 'crawled', 'timestamp': time.time()}, {'url': 'url_to_crawl', 'status': 'to_crawl', 'timestamp': time.time()}])\n",
    "    \n",
    "    # get all urls\n",
    "    local_mongo.collection.find({})\n",
    "    \n",
    "    urls = ['url_crawling', 'url_crawled', 'url_to_crawl', 'new_url']\n",
    "    local_mongo.collection.update_many({\"url\": {\"$in\": urls}, \"status\": \"crawling\"}, {\"$set\": {\"status\": \"to_crawl\"}})\n",
    "    # it should perform following operations:\n",
    "        * status:crawling <change it to to_crawl>\n",
    "        \n",
    "        * insert new_url with status:to_crawl\n",
    "        * update_many does not support upert=True, insert is not necessary since \n",
    "    \n",
    "    # Get all urls\n",
    "    list(local_mongo.collection.find({}))\n",
    "    \n",
    "    # Delete from mongo\n",
    "    local_mongo.collection.delete_many({'url': {'$in': urls}})\n",
    "    '''\n",
    "    while True:\n",
    "        timestamp = time.time() - 5 * 60  # 5 minutes\n",
    "        pipeline = [\n",
    "            {\"$match\": {\"status\": \"crawling\", \"timestamp\": {\"$lt\": str(timestamp)}}},\n",
    "            # {\"$limit\": limit}\n",
    "            # {\"$count\": \"count\"}\n",
    "        ]\n",
    "        expired_count = list(mongo.collection.aggregate(pipeline + [{\"$count\": \"count\"}]))\n",
    "        if expired_count:\n",
    "            expired_count = expired_count[0]['count']\n",
    "            no_iterations = int(expired_count / 10000) + 1\n",
    "            for _ in no_iterations:\n",
    "                expired_crawling_urls = list(mongo.collection.aggregate(pipeline + [{\"$limit\": 10000}]))\n",
    "                \n",
    "                # Save to local mongo: by update_many\n",
    "                # since url is unique, it would avoid duplicates\n",
    "                urls_expired = [entry['url'] for entry in expired_crawling_urls]\n",
    "                try:\n",
    "                    local_mongo.collection.update_many({\"url\": {\"$in\": urls_expired}, \"status\": \"crawling\"}, {\"$set\": {\"status\": \"to_crawl\"}})\n",
    "                except Exception as ex:\n",
    "                    pass\n",
    "                \n",
    "                # Delete from mongo online\n",
    "                try:\n",
    "                    mongo.collection.delete_many({'status': 'crawling', 'url': {'$in': urls_expired}})\n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                print(f\"recovered {len(urls_expired)} expired \\\"crawling\\\" urls from mongo -> local_mongo\")\n",
    "                logging.info(f\"recovered {len(urls_expired)} expired \\\"crawling\\\" urls from mongo -> local_mongo\")\n",
    "                \n",
    "        # sleep for 5 minutes\n",
    "        time.sleep(5 * 60)\n",
    "\n",
    "# Create and start the thread as a daemon\n",
    "revoke_crawling_url_thread = threading.Thread(target=revoke_crawling_url)\n",
    "revoke_crawling_url_thread.daemon = True\n",
    "revoke_crawling_url_thread.start()\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats():\n",
    "        '''\n",
    "            This is a thread to display the stats of the crawling process every 1 minute\n",
    "        '''\n",
    "        # while True:\n",
    "        # -----------------------------------------------------------------------\n",
    "        # --------------------------------- Stats ------------------------------- \n",
    "        # -----------------------------------------------------------------------\n",
    "        # there is no data\n",
    "        # length of to_crawl\n",
    "        to_crawl_count = mongo.collection.count_documents({\"status\": \"to_crawl\"})\n",
    "        to_crawl_spider_count = mongo.collection.count_documents({\"status\": 'to_crawl?'})\n",
    "        crawling_count = mongo.collection.count_documents({\"status\": \"crawling\"})\n",
    "        crawled_count = mongo.collection.count_documents({\"status\": \"crawled\"})\n",
    "        crawled_data_count = mongo.db['crawled_data'].count_documents({})\n",
    "        other_data_count = mongo.db['other_data'].count_documents({})\n",
    "        error_url_count = mongo.collection.count_documents({\"status\": \"error\"})\n",
    "        # to_crawl_sqlite_count = URLDatabase(db_path=\"urls.db\").count_entries(\"to_crawl\")\n",
    "        # crawled_sqlite_count = URLDatabase(db_path=\"urls.db\").count_entries(\"crawled\")\n",
    "\n",
    "        # Nice formatted view for to_crawl, crawled and crawling\n",
    "        # Formatted output\n",
    "        print(\"=================================================\")\n",
    "        print(\"#  *********** Crawl Queue Status ***********\")\n",
    "        print(\"=================================================\")\n",
    "        print(f'{time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}')\n",
    "        print(f\"\\\"to_crawl\\\" (Mongo): {locale.format_string('%d', to_crawl_count, grouping=True)}\")\n",
    "        print(f\"\\\"to_crawl?\\\" (Mongo by Spider): {locale.format_string('%d', to_crawl_spider_count, grouping=True)}\")\n",
    "        print(f\"Error (Mongo): {locale.format_string('%d', error_url_count, grouping=True)}\")\n",
    "        print(f\"Crawling (Mongo): {locale.format_string('%d', crawling_count, grouping=True)}\")\n",
    "        print(f\"Crawled_data (Mongo): {locale.format_string('%d', crawled_data_count, grouping=True)}\")\n",
    "        print(f\"Other_data (Mongo): {locale.format_string('%d', other_data_count, grouping=True)}\")\n",
    "        # print(f\"\\\"to_crawl\\\" (Sqlite): {locale.format_string('%d', to_crawl_sqlite_count, grouping=True)}\")\n",
    "        # print(f\"\\\"crawled\\\" (Sqlite): {locale.format_string('%d', crawled_sqlite_count, grouping=True)}\")\n",
    "        # print(f\"Crawled: {locale.format_string('%d', crawled_count, grouping=True)}\")\n",
    "        \n",
    "\n",
    "        \n",
    "        # Get mongo stats\n",
    "        # stats = mongo.db.command(\"dbstats\")\n",
    "        # Print the stats\n",
    "        # print(\"DB Size: \", stats['dataSize']/(1024*1024))\n",
    "        # print(\"Storage Size: \", stats['storageSize']/(1024*1024))\n",
    "        # print(\"Mongo Free Storage Space: \", stats['totalFreeStorageSize']/(1024*1024), end=\"\\n-----------------------------------------------\\n\")\n",
    "        \n",
    "        # Crawling Rate\n",
    "        # newly_crawled = crawled_count - number_of_links_crawled_at_start\n",
    "        # time_taken = time.time() - start_time\n",
    "        # crawling_rate = newly_crawled / time_taken\n",
    "        # print(f\"Crawling Rate: {locale.format_string('%d', crawling_rate, grouping=True)} links/sec\")\n",
    "        # expected_time_to_crawl = to_crawl_count / (crawling_rate if crawling_rate > 0 else 0.0000001)\n",
    "        # print(f\"Crawling Rate: {crawling_rate} links/sec\")\n",
    "        # print(f\"Expected Time to Crawl: {locale.format_string('%d', expected_time_to_crawl/(60*60*24), grouping=True)} days\")\n",
    "        \n",
    "        # Get Crawled File Size\n",
    "        print(f\"Size \\\"crawled_data.csv\\\": {os.path.getsize('crawled_data.csv')/(1024*1024) if os.path.exists('crawled_data.csv') else 0} MB\")\n",
    "        print(f\"Size of \\\"other_data.csv\\\": {os.path.getsize('other_data.csv')/(1024*1024) if os.path.exists('other_data.csv') else 0} MB\")\n",
    "        print(f\"Size of urls.db: {os.path.getsize('urls.db')/(1024*1024) if os.path.exists('urls.db') else 0} MB\")\n",
    "        print(\"===============================================\")\n",
    "\n",
    "        # -----------------------------------------------------------------------\n",
    "\n",
    "        # Sleep for 1 minute\n",
    "        time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_crawl_cleanup_and_mongo_to_crawl_refill():\n",
    "    '''\n",
    "        * This thread will run once every 1.5 hours? \n",
    "        - lets not allowmake it sleep, the delays from operations should be enouogh\n",
    "    '''\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # mograte \"to_crawl?\" from online mongo -> local_mongo\n",
    "    # -----------------------------------------------------\n",
    "    '''\n",
    "    * Get to_crawl? from online mongo\n",
    "    * loop over each url:\n",
    "        * insert if url does not exists locally\n",
    "        i.e. insert_many with ordered=False should do\n",
    "        ```\n",
    "            local_mongo.collection.insert_many(\n",
    "                [\n",
    "                    {\n",
    "                        'url': entry['url'],\n",
    "                        'status': 'to_crawl',\n",
    "                        'timestamp': entry['timestamp']\n",
    "                    } \n",
    "                    for entry in entries\n",
    "                ],\n",
    "                ordered=False)\n",
    "        ```\n",
    "        \n",
    "        ```\n",
    "            # Test\n",
    "            # pre existing data\n",
    "            local_mongo.collection.insert_one({'url': 'existing_url_with_status_crawled', 'status': 'crawled', 'timestamp': time.time()})\n",
    "\n",
    "            # new data\n",
    "            data = [\n",
    "                {'url': 'existing_url_with_status_crawled', 'status': 'to_crawl', 'timestamp': time.time()},\n",
    "                {'url': 'new_url_with_status_to_crawl', 'status': 'to_crawl', 'timestamp': time.time()}\n",
    "            ]\n",
    "\n",
    "            try:\n",
    "                local_mongo.collection.insert_many(data, ordered=False)\n",
    "            except Exception as bwe:\n",
    "                pass\n",
    "\n",
    "            # Get all data\n",
    "            list(local_mongo.collection.find({}))\n",
    "\n",
    "            # Delete from local mongo\n",
    "            local_mongo.collection.delete_many({'url': {'$in': ['existing_url_with_status_crawled', 'new_url_with_status_to_crawl']}})\n",
    "        ```\n",
    "    * delete from online mongo\n",
    "    '''\n",
    "    print(\"mongo_to_crawl_refill started\")\n",
    "    logging.info(\"mongo_to_crawl_refill started\")\n",
    "    while True:    \n",
    "        # +1 to avoid 0 division error and int() returns floor value. e.g. 1.9 -> 1\n",
    "        n_iterations = int(mongo.collection.count_documents({\"status\": 'to_crawl?'})/10000) + 1\n",
    "        for _ in list(range(n_iterations)):\n",
    "            # print(_)\n",
    "            # 10000 at a time\n",
    "            \n",
    "            # Get all urls from \"to_crawl?\"\n",
    "            entries = mongo.collection.find({\"status\": 'to_crawl?'}).limit(10000)\n",
    "            entries = list(entries)\n",
    "            # Save to local mongo\n",
    "            # print('pre-insert')\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                _ = local_mongo.collection.insert_many(\n",
    "                        [\n",
    "                            {\n",
    "                                'url': entry['url'],\n",
    "                                'status': 'to_crawl',\n",
    "                                'timestamp': entry['timestamp']\n",
    "                            } \n",
    "                            for entry in entries\n",
    "                        ],\n",
    "                        ordered=False)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            # print(f'inserted. time:{time.time()-start_time}, rate:{len(entries)/(time.time()-start_time)}')\n",
    "            # Delete from online mongo\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                _ = mongo.collection.delete_many(\n",
    "                    {\n",
    "                        'status': 'to_crawl?',\n",
    "                        'url': {'$in': [entry['url'] for entry in entries]}\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            # print(f'deleted. time:{time.time()-start_time}, rate:{len(entries)/(time.time()-start_time)}')\n",
    "            # print('deleted')\n",
    "            \n",
    "    \n",
    "    \n",
    "        print(f\"migrated {n_iterations*10000} \\\"to_crawl?\\\" from online_mongo to local_mongo\")\n",
    "        logging.info(f\"migrated {n_iterations*10000} \\\"to_crawl?\\\" from online_mongo to local_mongo\")\n",
    "        # ------------------------------------------------------------------------------------------------------------------------\n",
    "        # mongo_to_crawl_refill\n",
    "        # -----------------------\n",
    "        '''\n",
    "            * To maintain 50,000 - 60,000 to_crawl urls in online_mongo\n",
    "            * Get urls with `status=to_crawl` local_mongo\n",
    "            * insert_many to online mongo\n",
    "            * update_many `status:crawling` to in local_mongo\n",
    "\n",
    "            # Test\n",
    "            ```\n",
    "                # pre existing data\n",
    "                local_mongo.collection.insert_one({'url': 'existing_url_with_status_to_crawl', 'status': 'to_crawl', 'timestamp': time.time()})\n",
    "                local_mongo.collection.insert_one({'url': 'existing_url_with_status_crawling', 'status': 'crawling', 'timestamp': time.time()})\n",
    "                local_mongo.collection.insert_one({'url': 'existing_url_with_status_crawled', 'status': 'crawled', 'timestamp': time.time()})\n",
    "\n",
    "                # get urls from local_mongo\n",
    "                to_crawl_entries = list(local_mongo.collection.find({\"status\": 'to_crawl'}).limit(10))\n",
    "\n",
    "                # Insert to online mongo\n",
    "\n",
    "                # Update status in local_mongo\n",
    "                # no need to check status, since we are already filtering with status\n",
    "                local_mongo.collection.update_many(\n",
    "                    {\n",
    "                        'url': {'$in': [entry['url'] for entry in to_crawl_entries]},\n",
    "                    }, {'$set': {'status': 'crawling'}})\n",
    "\n",
    "                # get all data\n",
    "                list(local_mongo.collection.find({}))\n",
    "                # Delete data\n",
    "                local_mongo.collection.delete_many({})\n",
    "            ```\n",
    "        '''\n",
    "        online_to_crawl_count = mongo.collection.count_documents({\"status\": 'to_crawl'}) < 50000\n",
    "        required_to_crawl_count = 50000 - online_to_crawl_count\n",
    "        if required_to_crawl_count > 0:\n",
    "            # refill 10000 at a time\n",
    "            # to avoid max. sized reached error of mongo        \n",
    "            no_iterations = int(required_to_crawl_count/10000) + 1\n",
    "            for _ in range(no_iterations):\n",
    "                # Get urls from local_mongo\n",
    "                to_crawl_entries = list(local_mongo.collection.find({\"status\": 'to_crawl'}).limit(10000))\n",
    "                \n",
    "                # Insert to online mongo\n",
    "                try:\n",
    "                    mongo.collection.insert_many(\n",
    "                        [\n",
    "                            {\n",
    "                                'url': entry['url'],\n",
    "                                'status': 'to_crawl',\n",
    "                                'timestamp': entry['timestamp']\n",
    "                            } \n",
    "                            for entry in to_crawl_entries\n",
    "                        ],\n",
    "                        ordered=False)\n",
    "                except Exception as bwe:\n",
    "                    pass\n",
    "                \n",
    "                # Update status in local_mongo\n",
    "                local_mongo.collection.update_many(\n",
    "                        {'url': {'$in': [entry['url'] for entry in to_crawl_entries]},},\n",
    "                        {'$set': {'status': 'crawling'}}\n",
    "                    )\n",
    "            \n",
    "            print(f\"attempted inserting {no_iterations*10000} to_crawl to online_mongo\")\n",
    "            logging.info(f\"attempted inserting {no_iterations*10000} to_crawl to online_mongo\")\n",
    "            \n",
    "        # -------------------------------------------------------------------------------------------------------------------------\n",
    "        # Save error data to csv file from online_mongo\n",
    "        # ----------------------------------------------\n",
    "        no_iterations = int(mongo.collection.count_documents({'status': 'error'})/10000) + 1\n",
    "        for _ in no_iterations:\n",
    "            error_data = list(mongo.collection.find({'status': 'error'}).limit(10000))\n",
    "            formatted_for_csv = [{\n",
    "                    'url': error['url'],\n",
    "                    'timestamp': error['timestamp'],\n",
    "                    'status': error['status'],\n",
    "                    'status_code': error['status_code'] if 'status_code' in error else None,\n",
    "                    'error_type': error['error_type']\n",
    "                } for error in error_data]\n",
    "            # Append error data to csv\n",
    "            csv_file_path = 'error_data.csv'\n",
    "            file_exists = os.path.exists(csv_file_path)\n",
    "            with open(csv_file_path, 'a') as csvfile:\n",
    "                fieldnames = ['url', 'timestamp', 'status', 'status_code', 'error_type']\n",
    "                csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                if not file_exists:\n",
    "                    print(f\"creating csv file: {csv_file_path}\")\n",
    "                    # Write header only if file is empty\n",
    "                    csv_writer.writeheader()\n",
    "                # else:\n",
    "                #     print(f'csv file: \\\"{csv_file_path}\\\" exists')\n",
    "                csv_writer.writerows(formatted_for_csv)\n",
    "\n",
    "            print(f\"migrated {len(formatted_for_csv)} \\\"error?\\\" from mongo ->  {csv_file_path}\", end=\"\\n\\n\")\n",
    "\n",
    "            # Delete from mongo\n",
    "            mongo.collection.delete_many({'url': {'$in': [error['url'] for error in formatted_for_csv]}, 'status': 'error'})\n",
    "        # ------------------------------------------------------------------------------------------------------------------------\n",
    "        time.sleep(1 * 60)  # sleep for 1 minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 'crawling' to 'to_crawl' in local_mongo\n",
    "local_mongo.collection.update_many({\"status\": \"crawling\"}, {\"$set\": {\"status\": \"to_crawl\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_crawled_data():\n",
    "    # ---------------------------------------------------------------------------\n",
    "    ### Backup file named crawled_data.csv & other_data.csv after every 12 hours\n",
    "    # ---------------------------------------------------------------------------\n",
    "    while True:\n",
    "        # Backup once for every 12 hours\n",
    "        time.sleep(12 * 60 * 60)  # Sleep for 12 hours\n",
    "\n",
    "        # Check if 'crawled_data.csv' exists\n",
    "        if 'crawled_data.csv' in os.listdir():\n",
    "            # get latest backup index\n",
    "            backup_indices = [int(file.split('_')[-1].split('.')[0]) for file in os.listdir() if file.startswith('crawled_data_')]\n",
    "            print(backup_indices)\n",
    "            if backup_indices:\n",
    "                # Get maximum index\n",
    "                max_index = max(backup_indices)\n",
    "            else:\n",
    "                max_index = 0\n",
    "\n",
    "        try:\n",
    "            # backup 'crawled_data.csv' with index max_index+1\n",
    "            shutil.copy('crawled_data.csv', f'crawled_data_backup_{max_index+1}.csv')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            # log the error\n",
    "            logging.exception(ex)\n",
    "\n",
    "        try:\n",
    "            # backup 'other_data.csv' with index max_index+1\n",
    "            shutil.copy('other_data.csv', f'other_data_backup_{max_index+1}.csv')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            # log the error\n",
    "            logging.exception(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================================================\n",
    "# Save data from online mongo to csv file\n",
    "# ======================================================\n",
    "\n",
    "def crawled_data_consumer():\n",
    "    while True:\n",
    "        no_iterations = int(mongo.db['crawled_data'].count_documents({})/10000) + 1\n",
    "        for _ in no_iterations:\n",
    "            crawled_data = list(mongo.db['crawled_data'].find({}).limit(10000))\n",
    "            other_data = list(mongo.db['other_data'].find({}).limit(10000))\n",
    "            # print(f'{time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}: ', end='')\n",
    "            if crawled_data or other_data:\n",
    "                combined_data = {\"crawled_data\":crawled_data, \"other_data\":other_data}\n",
    "\n",
    "                # Save to .csv file\n",
    "                save_to_csv(combined_data)\n",
    "\n",
    "                # Delete multiple data by id\n",
    "                mongo.db['crawled_data'].delete_many({\"_id\": {\"$in\": [data['_id'] for data in crawled_data]} })\n",
    "                mongo.db['other_data'].delete_many({\"_id\": {\"$in\": [data_ot['_id'] for data_ot in other_data]} })\n",
    "            else:\n",
    "                sleep_duration = 10 # Sleep for 10 seconds\n",
    "                print('sleeping {sleep_duration} sec. : crawled_data_consumer')\n",
    "                time.sleep(sleep_duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_csv(data, data_type=\"crawled_data\"):\n",
    "    '''\n",
    "        * Save the crawled_data to 'csv' file\n",
    "        * update_many status of parent_url to 'crawled' in local_mongo\n",
    "        \n",
    "        * data_item['parent_url'] is data in format: url <str>\n",
    "        \n",
    "        # Test\n",
    "        ```\n",
    "            # Pre existing data\n",
    "            local_mongo.collection.insert_many([\n",
    "                {'url': 'url_crawling', 'status': 'crawling', 'timestamp': time.time()},\n",
    "                {'url': 'url_crawled', 'status': 'crawled', 'timestamp': time.time()},\n",
    "                {'url': 'url_to_crawl', 'status': 'to_crawl', 'timestamp': time.time()},\n",
    "                {'url': 'some_other_url', 'status': 'to_crawl', 'timestamp': time.time()}\n",
    "            ])\n",
    "\n",
    "            entries = ['url_crawling', 'url_crawled', 'url_to_crawl', 'new_url']\n",
    "\n",
    "            # update status of parent_url to 'crawled' in local_mongo                                       \n",
    "            local_mongo.collection.update_many(\n",
    "                {'url':{'$in': entries}},\n",
    "                {'$set': {'status': 'crawled'}},\n",
    "            )\n",
    "\n",
    "            # Display records\n",
    "            print(list(local_mongo.collection.find({})))\n",
    "\n",
    "            # Delete from local_mongo\n",
    "            local_mongo.collection.delete_many({})\n",
    "        ```\n",
    "    '''\n",
    "    for data_type, data_items in data.items():\n",
    "        '''\n",
    "            data_type: crawled_data, other_data\n",
    "        '''\n",
    "        csv_file_path = data_type + \".csv\"\n",
    "        if data_items:\n",
    "            # field_names = ['paragraph', 'parent_url', 'page_title', 'is_nepali_confidence']\n",
    "            field_names = data_items[0].keys()\n",
    "            file_exists = os.path.exists(csv_file_path)\n",
    "            # print(f'file_exists: {file_exists}')\n",
    "            # Open the CSV file in append mode\n",
    "            with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                # Create a CSV writer object\n",
    "                csv_writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "                \n",
    "                # If the file doesn't exist, write the header\n",
    "                if not file_exists:\n",
    "                    csv_writer.writeheader()\n",
    "                \n",
    "                try:\n",
    "                    # Get all unique crawled_urls\n",
    "                    entries = list(set([data_item['parent_url'] for data_item in data_items]))\n",
    "                    \n",
    "                    # update status of parent_url to 'crawled' in local_mongo\n",
    "                    local_mongo.collection.update_many(\n",
    "                        {'url':{'$in': entries}},\n",
    "                        {'$set': {'status': 'crawled'}},\n",
    "                    )\n",
    "                    \n",
    "                    # Save crawled data to csv file\n",
    "                    csv_writer.writerows(data_items)\n",
    "\n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    # log the error\n",
    "                    logging.exception(f'data_type:{data_type} exceptionL {ex}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_crawl_cleanup_and_mongo_to_crawl_refill_thread = threading.Thread(target=to_crawl_cleanup_and_mongo_to_crawl_refill)\n",
    "to_crawl_cleanup_and_mongo_to_crawl_refill_thread.daemon = True\n",
    "to_crawl_cleanup_and_mongo_to_crawl_refill_thread.start()\n",
    "\n",
    "display_stats_thread = threading.Thread(target=display_stats)\n",
    "display_stats_thread.daemon = True\n",
    "display_stats_thread.start()\n",
    "\n",
    "backup_thread = threading.Thread(target=backup_crawled_data)\n",
    "backup_thread.daemon = True\n",
    "backup_thread.start()\n",
    "\n",
    "crawled_data_consumer_thread = threading.Thread(target=crawled_data_consumer)\n",
    "crawled_data_consumer_thread.daemon = True   # daemon threads are forcefully shut down when Python exits and programme waits for non-daemon threads to finish their tasks.\n",
    "crawled_data_consumer_thread.start()\n",
    "\n",
    "# Start the thread\n",
    "# producer_thread.start()\n",
    "# publisher_thread.start()\n",
    "\n",
    "# Wait for both threads to finish\n",
    "# producer_thread.join()\n",
    "# publisher_thread.join()\n",
    "# consumer_thread.join()    # waits for consumer_thread to finigh\n",
    "revoke_crawling_url_thread.join()  # Wait for the thread to finish\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def producer():\n",
    "#     import random\n",
    "#     print('producer')\n",
    "#     data_paragraphs_copy = [{'paragraph': 'प्रदेश सरकार र निजी क्षेत्रको सहकार्यमा पहाडी तथा हिमाली क्षेत्रमा मनोरञ्जनात्मक तथा साहसिक पर्यटनको लागि हिलस्टेशनहरू विकास गर्न आवश्यक छ भन्दै उनले सांस्कृतिक, धार्मिक, साहसिक, कृषि, स्वास्थ्य तथा खेल पर्यटक आकर्षित गर्दै यस क्षेत्रको मौलिक संस्कृति संरक्षणमा महोत्सवले सहयोग गर्ने विश्वास व्यक्त गरे ।\\xa0', 'parent_url': 'https://hamrakura.com/news-details/161504/2023-12-27', 'page_title': 'लोकतन्त्रको आन्दोलन उठाउँदाकै आस्थाबाट निर्देशित छु – राष्ट्रपति पौडेल', 'is_nepali_confidence':'-1914.427728056908'},\n",
    "#         {'paragraph': 'त्रिवेणी बाहेक अन्य पालिकाबाट कुन-कुन घर परिवारले रकम पाउने भन्ने विवरण नआइसकेकाले ती पालिकाका लागि रकम निकासा भने हुन सकेको छैन ।', 'parent_url': 'https://hamrakura.com/news-details/159820/2023-11-28', 'page_title': 'भूकम्पपीडितको अस्थायी आवासका लागि रकम निकासा', 'is_nepali_confidence':'-800.0689898729324'},\n",
    "#         {'paragraph': 'निर्वाचित मण्डलले निर्वाचनका सबै प्रक्रिया अघि बढाएपनी सहमतिका लागि शीर्ष नेताहरूले समय मागेकाले निर्वाचन कमिटीले समय दिएको थियो । निर्वाचन कमिटीका संयोजक जगत बहादुर रोकायाले बताए ।','parent_url': 'https://hamrakura.com/news-details/160003/2023-12-01', 'page_title': 'अध्यक्ष मण्डलले घोषणा गरे जिल्ला कमिटी, टिके प्रथा चलाएको रावल पक्षको आरोप', 'is_nepali_confidence':'-1128.5258438587189'},\n",
    "#         {'paragraph': 'अहिलेसम्म एनसेलले शेयर किनबेच गरेको सम्बन्धमा नेपाल दूरसञ्चार प्राधिकरणले गरेको काम कारबाहीको सम्बन्धमा जानकारी माग्ने पत्र लेख्ने', 'parent_url':'https://hamrakura.com/news-details/161068/2023-12-19', 'page_title': 'एनसेलले राज्यलाई तिर्नुपर्ने कर असुल उपर गर्न सरकारलाई समितिको निर्देशन [भिडियो]', 'is_nepali_confidence':'-800.2218471765518'}\n",
    "#         ]\n",
    "#     while True:\n",
    "#         # add n items to the list\n",
    "#         n_items = random.randint(0, 3)\n",
    "#         data_paragraphs.extend(data_paragraphs_copy[:n_items])\n",
    "#         print(f'produced: {n_items}')\n",
    "\n",
    "#         # sleep randomly between 0 and 5 seconds\n",
    "#         time.sleep(random.randint(0, 5))\n",
    "\n",
    "# def publisher():\n",
    "#     print('published')\n",
    "#     while True:\n",
    "        \n",
    "#         if data_paragraphs:\n",
    "#             pushed = 0\n",
    "#             for paragraph in data_paragraphs:\n",
    "#                 push_to_redis('crawled_data', json.dumps(data_paragraphs.pop()))\n",
    "#                 pushed += 1\n",
    "#             print(f'==-published: {pushed}==-')\n",
    "#         else:\n",
    "#             time.sleep(5)  # sleep for a while before producing more items\n",
    "#         # time.sleep(1)  # sleep for a while before producing more items\n",
    "\n",
    "# def push_to_redis(list_name, data):\n",
    "#     redis_client.lpush(list_name, data)\n",
    "\n",
    "# def load_from_csv(csv_file_path=\"crawled_data.csv\"):\n",
    "#     data = []\n",
    "#     # Open the CSV file in read mode\n",
    "#     with open(csv_file_path, 'r', encoding='utf-8') as csvfile:\n",
    "#         # Create a CSV reader object\n",
    "#         csv_reader = csv.DictReader(csvfile)\n",
    "\n",
    "#         # Read and print each row of data\n",
    "#         for row in csv_reader:\n",
    "#             data.append(row)\n",
    "#             # print(row)\n",
    "#     return data\n",
    "\n",
    "# data_paragraphs = []\n",
    "\n",
    "# # Create producer and consumer threads\n",
    "# producer_thread = threading.Thread(target=producer)\n",
    "# publisher_thread = threading.Thread(target=publisher)\n",
    "\n",
    "# # Start the threads\n",
    "# producer_thread.start()\n",
    "# # consumer_thread.start()\n",
    "# publisher_thread.start()\n",
    "\n",
    "# # Wait for both threads to finish\n",
    "# producer_thread.join()\n",
    "# publisher_thread.join()\n",
    "# # consumer_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrationi from sqlite to local_mongo\n",
    "\n",
    "problem: about 2 million records in to_crawl and RAM is insufficient to load them all at a time.\n",
    "\n",
    "## The plan:\n",
    "* fetch 100k records from sqlite at a time\n",
    "* save them to local_mongo\n",
    "* delete from sqlite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlite_handler import URLDatabase\n",
    "from mongo import Mongo\n",
    "\n",
    "local_mongo = Mongo(local=True)\n",
    "sqlite_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# Empty local mongo\n",
    "local_mongo.collection.delete_many({})\n",
    "\n",
    "# for status in ['to_crawl', 'crawled']\n",
    "# Get 100000 urls from sqlite\n",
    "batch_size = 10000\n",
    "\n",
    "statuses = [\"crawled\", \"to_crawl\"]\n",
    "\n",
    "for status in statuses:\n",
    "    saved = 0\n",
    "    print('\\n --------------------------------------')\n",
    "    print(f'\\t status: {status}', end='\\n --------------------------------------')\n",
    "    entries = sqlite_db.fetch(status, batch_size)\n",
    "    while len(entries) > 0:\n",
    "        entries = [{'url':entry[0], 'timestamp': entry[1], 'status':status} for entry in entries]\n",
    "        \n",
    "        # Insert to local_mongo\n",
    "        try:\n",
    "            _ = local_mongo.collection.insert_many(entries, ordered=False)\n",
    "        except Exception as ex:\n",
    "            pass\n",
    "        \n",
    "        # # get all entries in local_mongo\n",
    "        # print(len(list(local_mongo.collection.find({}))))\n",
    "        \n",
    "        # # check if url exists before deleting\n",
    "        # sqlite_db.exists('crawled', 'https://onlinemajdoor.com/?cat=21')\n",
    "        \n",
    "        # Delete from sqlite\n",
    "        sqlite_db.delete(status, [entry['url'] for entry in entries])\n",
    "        \n",
    "        # # check if url exists after deleting\n",
    "        # sqlite_db.exists('crawled', 'https://onlinemajdoor.com/?cat=21')\n",
    "        \n",
    "        saved+=batch_size\n",
    "        print(f'saved {status}:{saved}')\n",
    "        \n",
    "        # update entries for next iteration\n",
    "        entries = sqlite_db.fetch(status, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## migrate from online mongoDB to local Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408348"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "mongo.collection.count_documents({})\n",
    "\n",
    "# Get status of all entries in collection\n",
    "mongo.collection.distinct('status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMpty to_crawl?\n",
    "entries = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "\n",
    "# Save to local mongo\n",
    "local_mongo.collection.insert_many(\n",
    "        [\n",
    "            {\n",
    "                'url': entry['url'],\n",
    "                'status': 'to_crawl',\n",
    "                'timestamp': entry['timestamp']\n",
    "            } \n",
    "            for entry in entries\n",
    "        ],\n",
    "        ordered=False)\n",
    "# Delete from online mongo\n",
    "mongo.collection.delete_many(\n",
    "    {\n",
    "        'status': 'to_crawl?',\n",
    "            'url': {'$in': [entry['url'] for entry in entries]}\n",
    "    })\n",
    "    \n",
    "    \n",
    "print(f\"migrated {len(entries)} \\\"to_crawl?\\\" from online_mongo to local_mongo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty crawled_data and other_data from online mongio\n",
    "crawled_count = mongo.db['crawled_data'].count_documents({})\n",
    "other_count = mongo.db['other_data'].count_documents({})\n",
    "max_data_count = max(crawled_count, other_count)\n",
    "if max_data_count > 0 :\n",
    "    no_iterations = int(max_data_count/10000) + 1\n",
    "    for _ in range(no_iterations):\n",
    "        crawled_data = list(mongo.db['crawled_data'].find({}).limit(10000))\n",
    "        other_data = list(mongo.db['other_data'].find({}).limit(10000))\n",
    "        # print(f'{time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}: ', end='')\n",
    "        if crawled_data or other_data:\n",
    "            combined_data = {\"crawled_data\":crawled_data, \"other_data\":other_data}\n",
    "\n",
    "            # Save to .csv file\n",
    "            save_to_csv(combined_data)\n",
    "            print(f\"crawled_data: saved {len(crawled_data)} crawled_data and {len(other_data)} other_data to csv file\")\n",
    "            # Delete multiple data by id\n",
    "            mongo.db['crawled_data'].delete_many({\"_id\": {\"$in\": [data['_id'] for data in crawled_data]} })\n",
    "            mongo.db['other_data'].delete_many({\"_id\": {\"$in\": [data_ot['_id'] for data_ot in other_data]} })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all to_crawl from mongo to local_mongo\n",
    "no_iterations = int(mongo.collection.count_documents({\"status\": 'to_crawl'})/10000) + 1\n",
    "for _ in range(no_iterations):\n",
    "    entries = list(mongo.collection.find({\"status\": 'to_crawl'}).limit(10000))\n",
    "    try:\n",
    "        local_mongo.collection.insert_many(\n",
    "            [\n",
    "                {\n",
    "                    'url': entry['url'],\n",
    "                    'status': 'to_crawl',\n",
    "                    'timestamp': entry['timestamp']\n",
    "                } \n",
    "                for entry in entries\n",
    "            ],\n",
    "            ordered=False)\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "    # Delete from online_mongo\n",
    "    _ = mongo.collection.delete_many({'status': 'to_crawl', 'url': {'$in': [entry['url'] for entry in entries]}})\n",
    "    print(f\"migrated {10000} \\\"to_crawl\\\" from online_mongo to local_mongo\")\n",
    "\n",
    "mongo.collection.count_documents({\"status\": 'to_crawl'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update all \"crawling\" from local_mongo to \"to_crawl\"\n",
    "local_mongo.collection.update_many({\"status\": \"crawling\"}, {\"$set\": {\"status\": \"to_crawl\"}})\n",
    "\n",
    "# get all crawling\n",
    "local_mongo.collection.count_documents({\"status\": \"crawling\"})\n",
    "\n",
    "# Delete \"to_crawl\" from online mongo\n",
    "mongo.collection.delete_many({\"status\": \"to_crawl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate to_crawl of online_mongo from local_mongo\n",
    "online_to_crawl_count = mongo.collection.count_documents({\"status\": 'to_crawl'}) < 50000\n",
    "required_to_crawl_count = 50000 - online_to_crawl_count\n",
    "if required_to_crawl_count > 0:\n",
    "    # refill 10000 at a time\n",
    "    # to avoid max. sized reached error of mongo        \n",
    "    no_iterations = int(required_to_crawl_count/10000) + 1\n",
    "    for _ in range(no_iterations):\n",
    "        print(f'iteration: {_}')\n",
    "        # Get urls from local_mongo\n",
    "        to_crawl_entries = list(local_mongo.collection.find({\"status\": 'to_crawl'}).limit(10000))\n",
    "        \n",
    "        # Insert to online mongo\n",
    "        try:\n",
    "            mongo.collection.insert_many(\n",
    "                [\n",
    "                    {\n",
    "                        'url': entry['url'],\n",
    "                        'status': 'to_crawl',\n",
    "                        'timestamp': entry['timestamp']\n",
    "                    } \n",
    "                    for entry in to_crawl_entries\n",
    "                ],\n",
    "                ordered=False)\n",
    "        except Exception as bwe:\n",
    "            pass\n",
    "        print(f'inserted {len(to_crawl_entries)} to_crawl to online_mongo')\n",
    "        # Update status in local_mongo\n",
    "        _ = local_mongo.collection.update_many(\n",
    "                {'url': {'$in': [entry['url'] for entry in to_crawl_entries]},},\n",
    "                {'$set': {'status': 'crawling'}}\n",
    "            )\n",
    "        print(f'updated local_mongo')\n",
    "    print(f\"attempted inserting {no_iterations*10000} to_crawl to online_mongo\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get urls with length greater than 1000\n",
    "from mongo import Mongo\n",
    "local_mongo = Mongo(local=True)\n",
    "local_mongo.collection.count_documents({\"url\": {\"$regex\": \".{1000,}\"}}) # 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_urls = list(collection.find({\"$where\": \"this.url.length > 1000\"}))\n",
    "len(long_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
