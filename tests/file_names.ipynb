{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "* md5 hash and uuid seem to be good options.\n",
    "* lets compare the speed of both.\n",
    "(md5 hash seems to be faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 100,000 iterations:\n",
      "\n",
      "MD5 Hash:\n",
      "  Average time: 1.36 microseconds\n",
      "  Std dev:     1.03 microseconds\n",
      "\n",
      "UUID:\n",
      "  Average time: 8.19 microseconds\n",
      "  Std dev:     4.44 microseconds\n",
      "\n",
      "UUID is 0.17x faster than MD5 hash\n",
      "\n",
      "Example outputs:\n",
      "MD5 Hash: 655eb937aace7ce932b572f303293d35\n",
      "UUID:    e4446d4a-6751-40a8-adfa-c1e24e089ecc\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import uuid\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from typing import List, Tuple\n",
    "\n",
    "def benchmark(iterations: int = 100000) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"Run benchmark comparing MD5 hash vs UUID generation.\"\"\"\n",
    "    url = \"https://www.example.com/very/long/path/with/parameters?param1=value1&param2=value2\" #*100\n",
    "    \n",
    "    hash_times = []\n",
    "    uuid_times = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Benchmark MD5\n",
    "        start = time.perf_counter()\n",
    "        hashlib.md5(url.encode()).hexdigest()\n",
    "        hash_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        # Benchmark UUID\n",
    "        start = time.perf_counter()\n",
    "        str(uuid.uuid4())\n",
    "        uuid_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return hash_times, uuid_times\n",
    "\n",
    "# Run benchmark\n",
    "iterations = 100000\n",
    "hash_times, uuid_times = benchmark(iterations)\n",
    "\n",
    "# Calculate statistics\n",
    "hash_mean = mean(hash_times) * 1000000  # Convert to microseconds\n",
    "uuid_mean = mean(uuid_times) * 1000000\n",
    "hash_std = stdev(hash_times) * 1000000\n",
    "uuid_std = stdev(uuid_times) * 1000000\n",
    "\n",
    "print(f\"Results over {iterations:,} iterations:\\n\")\n",
    "print(\"MD5 Hash:\")\n",
    "print(f\"  Average time: {hash_mean:.2f} microseconds\")\n",
    "print(f\"  Std dev:     {hash_std:.2f} microseconds\")\n",
    "print(\"\\nUUID:\")\n",
    "print(f\"  Average time: {uuid_mean:.2f} microseconds\")\n",
    "print(f\"  Std dev:     {uuid_std:.2f} microseconds\")\n",
    "\n",
    "print(f\"\\nUUID is {(hash_mean/uuid_mean):.2f}x faster than MD5 hash\")\n",
    "\n",
    "# Generate example outputs\n",
    "url = \"https://ekantipur.com/news/2025/01/07/tibet-earthquake-at-least-95-dead-more-than-130-injured-19-43.html\"\n",
    "print(\"\\nExample outputs:\")\n",
    "print(f\"MD5 Hash: {hashlib.md5(url.encode()).hexdigest()}\")\n",
    "print(f\"UUID:    {str(uuid.uuid4())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate file name using md5_hash\n",
    "lets create some pickle files with with hashed url  + timestamp as file name and contain dummy data and test our zipping code for those files. \n",
    "\n",
    "lets use uuid as zipped file name and create some temp files to make sure our code works as expected.\n",
    "\n",
    "\n",
    "This test suite:\n",
    "\n",
    "1. Creates pickle files with:\n",
    "   - URL hash + timestamp naming\n",
    "   - Both regular and temp files\n",
    "   - Dummy data inside each pickle\n",
    "\n",
    "2. Zips the files with:\n",
    "   - UUID as zip filename\n",
    "   - Excludes _temp.pickle files\n",
    "   - Includes a manifest\n",
    "\n",
    "3. Verifies:\n",
    "   - Temp files are excluded\n",
    "   - Files are properly compressed\n",
    "   - Manifest is included\n",
    "\n",
    "When you run it, you'll see:\n",
    "1. List of created regular and temp files\n",
    "2. The UUID-based zip filename\n",
    "3. Contents of the zip file\n",
    "4. Verification that temp files were excluded\n",
    "\n",
    "Would you like me to add any additional test cases or verification steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated filename: pickles/d22158c78143eeca7fa617577d741866_20250107_150032_616503.pickle\n",
      "\n",
      "All files for URL:\n",
      "- pickles/d22158c78143eeca7fa617577d741866_20250107_144244_573921.pickle\n",
      "- pickles/d22158c78143eeca7fa617577d741866_20250107_144244_674361.pickle\n",
      "- pickles/d22158c78143eeca7fa617577d741866_20250107_145149_330036.pickle\n",
      "- pickles/d22158c78143eeca7fa617577d741866_20250107_145149_372245_temp.pickle\n",
      "- pickles/d22158c78143eeca7fa617577d741866_20250107_150032_616696.pickle\n",
      "- pickles/d22158c78143eeca7fa617577d741866_20250107_150032_718720.pickle\n",
      "- pickles/d22158c78143eeca7fa617577d741866_20250107_150032_819726.pickle\n",
      "\n",
      "Latest file: pickles/d22158c78143eeca7fa617577d741866_20250107_150032_819726.pickle\n",
      "\n",
      "After cleanup:\n",
      "- pickles/d22158c78143eeca7fa617577d741866_20250107_150032_718720.pickle\n",
      "- pickles/d22158c78143eeca7fa617577d741866_20250107_150032_819726.pickle\n"
     ]
    }
   ],
   "source": [
    "# import hashlib\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "\n",
    "# class PickleFilePandit:\n",
    "#     '''\n",
    "#     * PickleFilePandit names the pickle files\n",
    "#     '''\n",
    "#     def __init__(self, base_dir: str = \"pickles\"):\n",
    "#         self.base_dir = Path(base_dir)\n",
    "#         self.base_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "#     def get_url_hash(self, url: str) -> str:\n",
    "#         \"\"\"Generate MD5 hash of URL.\"\"\"\n",
    "#         return hashlib.md5(url.encode()).hexdigest()\n",
    "    \n",
    "#     def generate_filename(self, url: str, include_date: bool = True) -> Path:\n",
    "#         \"\"\"Generate a unique filename for the URL.\"\"\"\n",
    "#         url_hash = self.get_url_hash(url)\n",
    "        \n",
    "#         if include_date:\n",
    "#             # Use timestamp for uniqueness and chronological ordering\n",
    "#             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "#             filename = f\"{url_hash}_{timestamp}.pickle\"\n",
    "#         else:\n",
    "#             # If you want to overwrite previous files for the same URL\n",
    "#             filename = f\"{url_hash}.pickle\"\n",
    "            \n",
    "#         return self.base_dir / filename\n",
    "    \n",
    "#     def get_files_for_url(self, url: str) -> list[Path]:\n",
    "#         \"\"\"Get all pickle files associated with a URL.\"\"\"\n",
    "#         url_hash = self.get_url_hash(url)\n",
    "#         return sorted(self.base_dir.glob(f\"{url_hash}_*.pickle\"))\n",
    "    \n",
    "#     def get_latest_file_for_url(self, url: str) -> Path | None:\n",
    "#         \"\"\"Get the most recent pickle file for a URL.\"\"\"\n",
    "#         files = self.get_files_for_url(url)\n",
    "#         return files[-1] if files else None\n",
    "    \n",
    "#     def cleanup_old_files(self, url: str, keep_latest_n: int = 5):\n",
    "#         \"\"\"Remove old pickle files, keeping only the n most recent ones.\"\"\"\n",
    "#         files = self.get_files_for_url(url)\n",
    "#         for file in files[:-keep_latest_n]:\n",
    "#             file.unlink()\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     pandit = PickleFilePandit()\n",
    "    \n",
    "#     # Example URL\n",
    "#     url = \"https://example.com/page1\"\n",
    "    \n",
    "#     # Generate filename\n",
    "#     pickle_path = pandit.generate_filename(url)\n",
    "#     print(f\"Generated filename: {pickle_path}\")\n",
    "    \n",
    "#     # Simulate multiple threads/processes saving files\n",
    "#     for _ in range(3):\n",
    "#         filename = pandit.generate_filename(url)\n",
    "#         with open(filename, 'wb') as f:\n",
    "#             f.write(b'test data')\n",
    "#         time.sleep(0.1)  # Simulate some processing time\n",
    "    \n",
    "#     # List all files for the URL\n",
    "#     print(\"\\nAll files for URL:\")\n",
    "#     for file in pandit.get_files_for_url(url):\n",
    "#         print(f\"- {file}\")\n",
    "    \n",
    "#     # Get latest file\n",
    "#     latest = pandit.get_latest_file_for_url(url)\n",
    "#     print(f\"\\nLatest file: {latest}\")\n",
    "    \n",
    "#     # Cleanup old files\n",
    "#     pandit.cleanup_old_files(url, keep_latest_n=2)\n",
    "#     print(\"\\nAfter cleanup:\")\n",
    "#     for file in pandit.get_files_for_url(url):\n",
    "#         print(f\"- {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test files...\n",
      "\n",
      "Regular files created:\n",
      "- d22158c78143eeca7fa617577d741866_20250107_150110_484189.pickle\n",
      "- ff824738a7790aa236b456ddb7f31593_20250107_150110_494624.pickle\n",
      "- 81f56e75d174cd01fe4bdf3e6e536da9_20250107_150110_505332.pickle\n",
      "- 355bff9458982feae78adad4f78ec912_20250107_150110_516223.pickle\n",
      "\n",
      "Temp files created:\n",
      "- d22158c78143eeca7fa617577d741866_20250107_150110_526903_temp.pickle\n",
      "- ff824738a7790aa236b456ddb7f31593_20250107_150110_537645_temp.pickle\n",
      "\n",
      "Zipping files...\n",
      "\n",
      "Created zip file: fb23bbc4-0190-4914-a6f4-82493ef76b70.zip\n",
      "\n",
      "Zip contents:\n",
      "- ff824738a7790aa236b456ddb7f31593_20250107_150110_494624.pickle\n",
      "- d22158c78143eeca7fa617577d741866_20250107_150110_484189.pickle\n",
      "- 355bff9458982feae78adad4f78ec912_20250107_150110_516223.pickle\n",
      "- 81f56e75d174cd01fe4bdf3e6e536da9_20250107_150110_505332.pickle\n",
      "\n",
      "Manifest contents:\n",
      "Backup created on: 2025-01-07 15:01:10.551214\n",
      "Total files: 4\n",
      "Files included:\n",
      "- ff824738a7790aa236b456ddb7f31593_20250107_150110_494624.pickle\n",
      "- d22158c78143eeca7fa617577d741866_20250107_150110_484189.pickle\n",
      "- 355bff9458982feae78adad4f78ec912_20250107_150110_516223.pickle\n",
      "- 81f56e75d174cd01fe4bdf3e6e536da9_20250107_150110_505332.pickle\n",
      "\n",
      "\n",
      "Verifying exclusion of temp files...\n",
      "Success: Temp file d22158c78143eeca7fa617577d741866_20250107_150110_526903_temp.pickle was correctly excluded!\n",
      "Success: Temp file ff824738a7790aa236b456ddb7f31593_20250107_150110_537645_temp.pickle was correctly excluded!\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import random\n",
    "\n",
    "class PickleTestSuite:\n",
    "    def __init__(self, base_dir: str = \"pickles\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def generate_url_hash(self, url: str) -> str:\n",
    "        \"\"\"Generate MD5 hash of URL.\"\"\"\n",
    "        return hashlib.md5(url.encode()).hexdigest()\n",
    "    \n",
    "    def create_pickle_filename(self, url: str, is_temp: bool = False) -> str:\n",
    "        \"\"\"Create filename with hash and timestamp.\"\"\"\n",
    "        url_hash = self.generate_url_hash(url)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        suffix = \"_temp.pickle\" if is_temp else \".pickle\"\n",
    "        return f\"{url_hash}_{timestamp}{suffix}\"\n",
    "    \n",
    "    def create_dummy_data(self) -> dict:\n",
    "        \"\"\"Create some dummy data to pickle.\"\"\"\n",
    "        return {\n",
    "            'timestamp': datetime.now(),\n",
    "            'random_number': random.randint(1, 1000),\n",
    "            'sample_text': f\"Sample data {random.randint(1, 100)}\",\n",
    "            'metrics': {\n",
    "                'value1': random.random(),\n",
    "                'value2': random.random()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_test_files(self, num_regular: int = 3, num_temp: int = 2) -> tuple[list[Path], list[Path]]:\n",
    "        \"\"\"Create test pickle files.\"\"\"\n",
    "        regular_files = []\n",
    "        temp_files = []\n",
    "        \n",
    "        # Sample URLs\n",
    "        urls = [\n",
    "            \"https://example.com/page1\",\n",
    "            \"https://example.com/page2\",\n",
    "            \"https://example.com/page3\",\n",
    "            \"https://example.com/page4\",\n",
    "            \"https://example.com/page5\"\n",
    "        ]\n",
    "        \n",
    "        # Create regular files\n",
    "        for i in range(num_regular):\n",
    "            url = urls[i % len(urls)]\n",
    "            filename = self.create_pickle_filename(url)\n",
    "            filepath = self.base_dir / filename\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(self.create_dummy_data(), f)\n",
    "            \n",
    "            regular_files.append(filepath)\n",
    "            time.sleep(0.01)  # Ensure unique timestamps\n",
    "        \n",
    "        # Create temp files\n",
    "        for i in range(num_temp):\n",
    "            url = urls[i % len(urls)]\n",
    "            filename = self.create_pickle_filename(url, is_temp=True)\n",
    "            filepath = self.base_dir / filename\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(self.create_dummy_data(), f)\n",
    "            \n",
    "            temp_files.append(filepath)\n",
    "            time.sleep(0.01)  # Ensure unique timestamps\n",
    "        \n",
    "        return regular_files, temp_files\n",
    "\n",
    "def zip_pickles(pickle_dir: str = \"pickles\") -> str:\n",
    "    \"\"\"Zip pickle files using UUID as filename.\"\"\"\n",
    "    zip_filename = f\"{uuid.uuid4()}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        pickle_path = Path(pickle_dir)\n",
    "        \n",
    "        # Get all non-temp pickle files\n",
    "        pickle_files = [\n",
    "            f for f in pickle_path.glob(\"*.pickle\")\n",
    "            if not f.name.endswith(\"_temp.pickle\")\n",
    "        ]\n",
    "        \n",
    "        # Add files to zip\n",
    "        for file in pickle_files:\n",
    "            zipf.write(file, file.name)\n",
    "        \n",
    "        # Add manifest\n",
    "        manifest = f\"\"\"Backup created on: {datetime.now()}\n",
    "                    Total files: {len(pickle_files)}\n",
    "                    Files included:\n",
    "                    {chr(10).join(f'- {f.name}' for f in pickle_files)}\n",
    "                    \"\"\"\n",
    "        zipf.writestr(\"manifest.txt\", manifest)\n",
    "    \n",
    "    return zip_filename\n",
    "\n",
    "# Run test\n",
    "if __name__ == \"__main__\":\n",
    "    # Create test suite\n",
    "    test_suite = PickleTestSuite()\n",
    "    \n",
    "    # Create test files\n",
    "    print(\"Creating test files...\")\n",
    "    regular_files, temp_files = test_suite.create_test_files(num_regular=4, num_temp=2)\n",
    "    \n",
    "    print(\"\\nRegular files created:\")\n",
    "    for file in regular_files:\n",
    "        print(f\"- {file.name}\")\n",
    "    \n",
    "    print(\"\\nTemp files created:\")\n",
    "    for file in temp_files:\n",
    "        print(f\"- {file.name}\")\n",
    "    \n",
    "    # Zip the files\n",
    "    print(\"\\nZipping files...\")\n",
    "    zip_file = zip_pickles()\n",
    "    \n",
    "    # Show zip contents\n",
    "    print(f\"\\nCreated zip file: {zip_file}\")\n",
    "    print(\"\\nZip contents:\")\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zipf:\n",
    "        for file in zipf.namelist():\n",
    "            if file == \"manifest.txt\":\n",
    "                print(\"\\nManifest contents:\")\n",
    "                print(zipf.read(file).decode())\n",
    "            else:\n",
    "                print(f\"- {file}\")\n",
    "    \n",
    "    # Verify temp files were excluded\n",
    "    print(\"\\nVerifying exclusion of temp files...\")\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zipf:\n",
    "        files_in_zip = zipf.namelist()\n",
    "        for temp_file in temp_files:\n",
    "            if temp_file.name in files_in_zip:\n",
    "                print(f\"Warning: Temp file {temp_file.name} was incorrectly included!\")\n",
    "            else:\n",
    "                print(f\"Success: Temp file {temp_file.name} was correctly excluded!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
